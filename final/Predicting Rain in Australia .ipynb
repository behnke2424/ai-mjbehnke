{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "Keras: 2.3.1\n",
      "Pandas: 1.0.1\n",
      "Sklearn: 0.22.1\n",
      "Numpy: 1.18.1\n"
     ]
    }
   ],
   "source": [
    "# The libraries that I am using\n",
    "import sys\n",
    "import sklearn\n",
    "import keras\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "print('Python: {0}'.format(sys.version))\n",
    "print(\"Keras: {0}\".format(keras.__version__))\n",
    "print(\"Pandas: {0}\".format(pandas.__version__))\n",
    "print(\"Sklearn: {0}\".format(sklearn.__version__))\n",
    "print(\"Numpy: {0}\".format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set Dimensions:  Rows = 142193 | Columns = 24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RISK_MM</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86001</th>\n",
       "      <td>2012-02-20</td>\n",
       "      <td>7</td>\n",
       "      <td>23.1</td>\n",
       "      <td>30.1</td>\n",
       "      <td>31.8</td>\n",
       "      <td>8.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>1009.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>29.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83201</th>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>28</td>\n",
       "      <td>13.8</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1019.9</td>\n",
       "      <td>1014.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>23.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85553</th>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>7</td>\n",
       "      <td>20.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>11.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1014.1</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.3</td>\n",
       "      <td>29.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62780</th>\n",
       "      <td>2009-02-14</td>\n",
       "      <td>40</td>\n",
       "      <td>10.5</td>\n",
       "      <td>27.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>12.3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1022.8</td>\n",
       "      <td>1019.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72802</th>\n",
       "      <td>2009-02-16</td>\n",
       "      <td>27</td>\n",
       "      <td>15.2</td>\n",
       "      <td>23.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>12.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1019.7</td>\n",
       "      <td>1017.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>22.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103016</th>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>42</td>\n",
       "      <td>21.7</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1009.8</td>\n",
       "      <td>1009.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.1</td>\n",
       "      <td>34.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34330</th>\n",
       "      <td>2013-05-05</td>\n",
       "      <td>19</td>\n",
       "      <td>12.6</td>\n",
       "      <td>18.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>1022.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129782</th>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>20</td>\n",
       "      <td>10.7</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1022.5</td>\n",
       "      <td>1016.7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91275</th>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>18</td>\n",
       "      <td>23.7</td>\n",
       "      <td>31.2</td>\n",
       "      <td>40.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1013.1</td>\n",
       "      <td>1009.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>29.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99359</th>\n",
       "      <td>2015-10-08</td>\n",
       "      <td>48</td>\n",
       "      <td>2.3</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>9.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1035.4</td>\n",
       "      <td>1030.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date  Location  MinTemp  MaxTemp  Rainfall  Evaporation  \\\n",
       "86001   2012-02-20         7     23.1     30.1      31.8          8.2   \n",
       "83201   2012-09-28        28     13.8     25.5       0.0          4.8   \n",
       "85553   2010-10-30         7     20.5     30.1       0.0          8.8   \n",
       "62780   2009-02-14        40     10.5     27.3       0.0          5.8   \n",
       "72802   2009-02-16        27     15.2     23.7       0.0          6.8   \n",
       "103016  2009-02-08        42     21.7     35.3       0.0         24.0   \n",
       "34330   2013-05-05        19     12.6     18.1       0.0          6.4   \n",
       "129782  2017-04-07        20     10.7     25.0       0.0          3.0   \n",
       "91275   2010-02-26        18     23.7     31.2      40.6          8.6   \n",
       "99359   2015-10-08        48      2.3     21.8       0.0          3.2   \n",
       "\n",
       "        Sunshine  WindGustDir  WindGustSpeed  WindDir9am  ...  Humidity3pm  \\\n",
       "86001        5.3         11.0           24.0        11.0  ...         70.0   \n",
       "83201       10.8          5.0           28.0         4.0  ...         59.0   \n",
       "85553       11.1          3.0           37.0        11.0  ...         53.0   \n",
       "62780       12.3         11.0           41.0        14.0  ...         26.0   \n",
       "72802       12.1         10.0           37.0         3.0  ...         61.0   \n",
       "103016      12.1         12.0           67.0        11.0  ...         25.0   \n",
       "34330        9.1          9.0           43.0        16.0  ...         42.0   \n",
       "129782       8.8          7.0           65.0         4.0  ...         24.0   \n",
       "91275        6.7          1.0           54.0        10.0  ...         76.0   \n",
       "99359        9.9          3.0           39.0         1.0  ...         40.0   \n",
       "\n",
       "        Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  \\\n",
       "86001        1013.0       1009.5       7.0       6.0     26.9     29.2   \n",
       "83201        1019.9       1014.8       1.0       1.0     22.5     23.9   \n",
       "85553        1014.1       1010.8       2.0       3.0     27.3     29.4   \n",
       "62780        1022.8       1019.3       0.0       0.0     16.0     26.4   \n",
       "72802        1019.7       1017.1       6.0       0.0     17.4     22.1   \n",
       "103016       1009.8       1009.5       1.0       1.0     22.1     34.3   \n",
       "34330        1025.2       1022.3       6.0       1.0     15.0     17.8   \n",
       "129782       1022.5       1016.7       7.0       8.0     15.2     24.2   \n",
       "91275        1013.1       1009.8       7.0       7.0     26.6     29.2   \n",
       "99359        1035.4       1030.3       0.0       1.0     10.3     20.8   \n",
       "\n",
       "        RainToday  RISK_MM  RainTomorrow  \n",
       "86001         1.0      0.0             0  \n",
       "83201         0.0      0.0             0  \n",
       "85553         0.0      0.0             0  \n",
       "62780         0.0      0.0             0  \n",
       "72802         0.0      0.0             0  \n",
       "103016        0.0      0.0             0  \n",
       "34330         0.0      0.0             0  \n",
       "129782        0.0      0.0             0  \n",
       "91275         1.0     48.6             1  \n",
       "99359         0.0      0.0             0  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Read in the Dataset csv file using pandas\n",
    "dataFrame = pd.read_csv('weatherAUS.csv')\n",
    "\n",
    "# The Data, rows define entries while the columns define the parameters\n",
    "rows, columns = dataFrame.shape\n",
    "print(\"Data Set Dimensions:  Rows = {1} | Columns = {0}\".format(columns, rows))\n",
    "\n",
    "# Replace Final results from Yes/No to 1/0 respectively\n",
    "dataFrame['RainTomorrow'].replace('No', 0, inplace=True)\n",
    "dataFrame['RainTomorrow'].replace('Yes', 1, inplace=True)\n",
    "dataFrame['RainToday'].replace('No', 0, inplace=True)\n",
    "dataFrame['RainToday'].replace('Yes', 1, inplace=True)\n",
    "\n",
    "# Change Wind Gusts/Directions to have numeric values\n",
    "dataFrame['WindGustDir'].replace('E', 1, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('E', 1, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('E', 1, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('ENE', 2, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('ENE', 2, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('ENE', 2, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('ESE', 3, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('ESE', 3, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('ESE', 3, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('N', 4, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('N', 4, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('N', 4, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('NE', 5, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('NE', 5, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('NE', 5, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('NNE', 6, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('NNE', 6, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('NNE', 6, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('NNW', 7, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('NNW', 7, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('NNW', 7, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('NW', 8, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('NW', 8, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('NW', 8, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('S', 9, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('S', 9, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('S', 9, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('SE', 10, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('SE', 10, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('SE', 10, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('SSE', 11, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('SSE', 11, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('SSE', 11, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('SSW', 12, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('SSW', 12, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('SSW', 12, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('SW', 13, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('SW', 13, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('SW', 13, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('W', 14, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('W', 14, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('W', 14, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('WNW', 15, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('WNW', 15, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('WNW', 15, inplace=True)\n",
    "dataFrame['WindGustDir'].replace('WSW', 16, inplace=True)\n",
    "dataFrame['WindDir9am'].replace('WSW', 16, inplace=True)\n",
    "dataFrame['WindDir3pm'].replace('WSW', 16, inplace=True)\n",
    "\n",
    "# Add all locations to a set\n",
    "locationSet = set()\n",
    "locationIndex = 0\n",
    "for location in dataFrame['Location']:\n",
    "    locationSet.add(location)\n",
    "\n",
    "# Replace every location with a consecutive number\n",
    "locationIndex = 0\n",
    "for location in locationSet:\n",
    "    dataFrame['Location'].replace(location, locationIndex, inplace=True)\n",
    "    locationIndex += 1\n",
    "\n",
    "# Delete all rows that contain NA values\n",
    "dataFrame.dropna(inplace=True)\n",
    "\n",
    "# Shuffle data so it is not sorted by location\n",
    "dataFrame = shuffle(dataFrame)\n",
    "\n",
    "# A snapshot of what our data looks like\n",
    "dataFrame[0:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RISK_MM</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "      <td>56420.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.585980</td>\n",
       "      <td>13.464770</td>\n",
       "      <td>24.219206</td>\n",
       "      <td>2.130397</td>\n",
       "      <td>5.503135</td>\n",
       "      <td>7.735626</td>\n",
       "      <td>8.485537</td>\n",
       "      <td>40.877366</td>\n",
       "      <td>8.122935</td>\n",
       "      <td>8.574424</td>\n",
       "      <td>...</td>\n",
       "      <td>49.601985</td>\n",
       "      <td>1017.239505</td>\n",
       "      <td>1014.795580</td>\n",
       "      <td>4.241705</td>\n",
       "      <td>4.326515</td>\n",
       "      <td>18.204961</td>\n",
       "      <td>22.710333</td>\n",
       "      <td>0.220879</td>\n",
       "      <td>2.346960</td>\n",
       "      <td>0.220259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.317428</td>\n",
       "      <td>6.416689</td>\n",
       "      <td>6.970676</td>\n",
       "      <td>7.014822</td>\n",
       "      <td>3.696282</td>\n",
       "      <td>3.758153</td>\n",
       "      <td>4.792305</td>\n",
       "      <td>13.335232</td>\n",
       "      <td>4.660823</td>\n",
       "      <td>4.711700</td>\n",
       "      <td>...</td>\n",
       "      <td>20.197040</td>\n",
       "      <td>6.909357</td>\n",
       "      <td>6.870892</td>\n",
       "      <td>2.797162</td>\n",
       "      <td>2.647251</td>\n",
       "      <td>6.567991</td>\n",
       "      <td>6.836543</td>\n",
       "      <td>0.414843</td>\n",
       "      <td>8.731885</td>\n",
       "      <td>0.414425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>-6.700000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>980.500000</td>\n",
       "      <td>977.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1012.700000</td>\n",
       "      <td>1010.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1017.200000</td>\n",
       "      <td>1014.700000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>29.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>1021.800000</td>\n",
       "      <td>1019.400000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>27.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>48.100000</td>\n",
       "      <td>206.200000</td>\n",
       "      <td>81.200000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1040.400000</td>\n",
       "      <td>1038.900000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>39.400000</td>\n",
       "      <td>46.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>367.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Location       MinTemp       MaxTemp      Rainfall   Evaporation  \\\n",
       "count  56420.000000  56420.000000  56420.000000  56420.000000  56420.000000   \n",
       "mean      23.585980     13.464770     24.219206      2.130397      5.503135   \n",
       "std       14.317428      6.416689      6.970676      7.014822      3.696282   \n",
       "min        2.000000     -6.700000      4.100000      0.000000      0.000000   \n",
       "25%       14.000000      8.600000     18.700000      0.000000      2.800000   \n",
       "50%       21.000000     13.200000     23.900000      0.000000      5.000000   \n",
       "75%       40.000000     18.400000     29.700000      0.600000      7.400000   \n",
       "max       48.000000     31.400000     48.100000    206.200000     81.200000   \n",
       "\n",
       "           Sunshine   WindGustDir  WindGustSpeed    WindDir9am    WindDir3pm  \\\n",
       "count  56420.000000  56420.000000   56420.000000  56420.000000  56420.000000   \n",
       "mean       7.735626      8.485537      40.877366      8.122935      8.574424   \n",
       "std        3.758153      4.792305      13.335232      4.660823      4.711700   \n",
       "min        0.000000      1.000000       9.000000      1.000000      1.000000   \n",
       "25%        5.000000      4.000000      31.000000      4.000000      4.000000   \n",
       "50%        8.600000      9.000000      39.000000      8.000000      9.000000   \n",
       "75%       10.700000     13.000000      48.000000     12.000000     13.000000   \n",
       "max       14.500000     16.000000     124.000000     16.000000     16.000000   \n",
       "\n",
       "       ...   Humidity3pm   Pressure9am   Pressure3pm      Cloud9am  \\\n",
       "count  ...  56420.000000  56420.000000  56420.000000  56420.000000   \n",
       "mean   ...     49.601985   1017.239505   1014.795580      4.241705   \n",
       "std    ...     20.197040      6.909357      6.870892      2.797162   \n",
       "min    ...      0.000000    980.500000    977.100000      0.000000   \n",
       "25%    ...     35.000000   1012.700000   1010.100000      1.000000   \n",
       "50%    ...     50.000000   1017.200000   1014.700000      5.000000   \n",
       "75%    ...     63.000000   1021.800000   1019.400000      7.000000   \n",
       "max    ...    100.000000   1040.400000   1038.900000      8.000000   \n",
       "\n",
       "           Cloud3pm       Temp9am       Temp3pm     RainToday       RISK_MM  \\\n",
       "count  56420.000000  56420.000000  56420.000000  56420.000000  56420.000000   \n",
       "mean       4.326515     18.204961     22.710333      0.220879      2.346960   \n",
       "std        2.647251      6.567991      6.836543      0.414843      8.731885   \n",
       "min        0.000000     -0.700000      3.700000      0.000000      0.000000   \n",
       "25%        2.000000     13.100000     17.400000      0.000000      0.000000   \n",
       "50%        5.000000     17.800000     22.400000      0.000000      0.000000   \n",
       "75%        7.000000     23.300000     27.900000      0.000000      0.600000   \n",
       "max        9.000000     39.400000     46.100000      1.000000    367.600000   \n",
       "\n",
       "       RainTomorrow  \n",
       "count  56420.000000  \n",
       "mean       0.220259  \n",
       "std        0.414425  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some stats about our data file\n",
    "dataFrame.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2012-02-20' 7 23.1 ... 1.0 0.0 0]\n",
      " ['2012-09-28' 28 13.8 ... 0.0 0.0 0]\n",
      " ['2010-10-30' 7 20.5 ... 0.0 0.0 0]\n",
      " ...\n",
      " ['2011-03-16' 48 7.2 ... 0.0 0.0 0]\n",
      " ['2010-12-06' 19 19.6 ... 1.0 0.8 0]\n",
      " ['2009-01-26' 44 19.6 ... 0.0 0.0 0]]\n",
      "(56420, 24)\n"
     ]
    }
   ],
   "source": [
    "# Convert our dataFrame to a numpy array\n",
    "setOfData = dataFrame.values\n",
    "print(setOfData)\n",
    "print(setOfData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42 21.7 35.3 0.0 24.0 12.1 12.0 67.0 11.0 11.0 37.0 31.0 54.0 25.0\n",
      "  1009.8 1009.5 1.0 1.0 22.1 34.3 0.0 0.0]\n",
      " [19 12.6 18.1 0.0 6.4 9.1 9.0 43.0 16.0 11.0 11.0 19.0 58.0 42.0 1025.2\n",
      "  1022.3 6.0 1.0 15.0 17.8 0.0 0.0]\n",
      " [20 10.7 25.0 0.0 3.0 8.8 7.0 65.0 4.0 4.0 11.0 31.0 63.0 24.0 1022.5\n",
      "  1016.7 7.0 8.0 15.2 24.2 0.0 0.0]\n",
      " [18 23.7 31.2 40.6 8.6 6.7 1.0 54.0 10.0 3.0 33.0 30.0 81.0 76.0 1013.1\n",
      "  1009.8 7.0 7.0 26.6 29.2 1.0 48.6]\n",
      " [48 2.3 21.8 0.0 3.2 9.9 3.0 39.0 1.0 3.0 26.0 11.0 73.0 40.0 1035.4\n",
      "  1030.3 0.0 1.0 10.3 20.8 0.0 0.0]]\n",
      "[0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into input and output (inData, outData respectively)\n",
    "inData = setOfData[:,1:23] # Do not choose the date or location column\n",
    "outData = setOfData[:,23]\n",
    "print(inData[5:10])\n",
    "print(outData[5:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X=inData)\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "      <td>5.642000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-8.056890e-17</td>\n",
       "      <td>2.219423e-16</td>\n",
       "      <td>3.626734e-16</td>\n",
       "      <td>-1.928524e-15</td>\n",
       "      <td>1.212912e-16</td>\n",
       "      <td>3.531068e-16</td>\n",
       "      <td>-2.294513e-16</td>\n",
       "      <td>-1.121759e-15</td>\n",
       "      <td>-1.298028e-16</td>\n",
       "      <td>4.271781e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.001975e-16</td>\n",
       "      <td>8.549819e-17</td>\n",
       "      <td>4.202553e-15</td>\n",
       "      <td>2.146056e-14</td>\n",
       "      <td>3.214695e-16</td>\n",
       "      <td>-2.014714e-15</td>\n",
       "      <td>6.301431e-17</td>\n",
       "      <td>-3.205998e-17</td>\n",
       "      <td>-1.324967e-15</td>\n",
       "      <td>-7.500922e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "      <td>1.000009e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.507685e+00</td>\n",
       "      <td>-3.142579e+00</td>\n",
       "      <td>-2.886289e+00</td>\n",
       "      <td>-3.037021e-01</td>\n",
       "      <td>-1.488843e+00</td>\n",
       "      <td>-2.058376e+00</td>\n",
       "      <td>-1.562005e+00</td>\n",
       "      <td>-2.390483e+00</td>\n",
       "      <td>-1.528270e+00</td>\n",
       "      <td>-1.607592e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.558239e+00</td>\n",
       "      <td>-2.455925e+00</td>\n",
       "      <td>-5.317403e+00</td>\n",
       "      <td>-5.486320e+00</td>\n",
       "      <td>-1.516445e+00</td>\n",
       "      <td>-1.634357e+00</td>\n",
       "      <td>-2.878373e+00</td>\n",
       "      <td>-2.780718e+00</td>\n",
       "      <td>-5.324452e-01</td>\n",
       "      <td>-2.687829e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.695382e-01</td>\n",
       "      <td>-7.581500e-01</td>\n",
       "      <td>-7.917818e-01</td>\n",
       "      <td>-3.037021e-01</td>\n",
       "      <td>-7.313184e-01</td>\n",
       "      <td>-7.279240e-01</td>\n",
       "      <td>-9.359957e-01</td>\n",
       "      <td>-7.407036e-01</td>\n",
       "      <td>-8.846016e-01</td>\n",
       "      <td>-9.708734e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.873737e-01</td>\n",
       "      <td>-7.229829e-01</td>\n",
       "      <td>-6.570142e-01</td>\n",
       "      <td>-6.834078e-01</td>\n",
       "      <td>-1.158937e+00</td>\n",
       "      <td>-8.788497e-01</td>\n",
       "      <td>-7.772553e-01</td>\n",
       "      <td>-7.767640e-01</td>\n",
       "      <td>-5.324452e-01</td>\n",
       "      <td>-2.687829e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.806192e-01</td>\n",
       "      <td>-4.126302e-02</td>\n",
       "      <td>-4.579309e-02</td>\n",
       "      <td>-3.037021e-01</td>\n",
       "      <td>-1.361205e-01</td>\n",
       "      <td>2.300018e-01</td>\n",
       "      <td>1.073528e-01</td>\n",
       "      <td>-1.407837e-01</td>\n",
       "      <td>-2.637650e-02</td>\n",
       "      <td>9.032404e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>6.081509e-02</td>\n",
       "      <td>1.970677e-02</td>\n",
       "      <td>-5.717731e-03</td>\n",
       "      <td>-1.391092e-02</td>\n",
       "      <td>2.710968e-01</td>\n",
       "      <td>2.544113e-01</td>\n",
       "      <td>-6.165730e-02</td>\n",
       "      <td>-4.539370e-02</td>\n",
       "      <td>-5.324452e-01</td>\n",
       "      <td>-2.687829e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.146447e+00</td>\n",
       "      <td>7.691310e-01</td>\n",
       "      <td>7.862713e-01</td>\n",
       "      <td>-2.181681e-01</td>\n",
       "      <td>5.131863e-01</td>\n",
       "      <td>7.887918e-01</td>\n",
       "      <td>9.420317e-01</td>\n",
       "      <td>5.341262e-01</td>\n",
       "      <td>8.318486e-01</td>\n",
       "      <td>9.392820e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>7.090039e-01</td>\n",
       "      <td>6.633712e-01</td>\n",
       "      <td>6.600520e-01</td>\n",
       "      <td>6.701403e-01</td>\n",
       "      <td>9.861134e-01</td>\n",
       "      <td>1.009919e+00</td>\n",
       "      <td>7.757447e-01</td>\n",
       "      <td>7.591136e-01</td>\n",
       "      <td>-5.324452e-01</td>\n",
       "      <td>-2.000686e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.705211e+00</td>\n",
       "      <td>2.795116e+00</td>\n",
       "      <td>3.425924e+00</td>\n",
       "      <td>2.909146e+01</td>\n",
       "      <td>2.047937e+01</td>\n",
       "      <td>1.799936e+00</td>\n",
       "      <td>1.568041e+00</td>\n",
       "      <td>6.233365e+00</td>\n",
       "      <td>1.690074e+00</td>\n",
       "      <td>1.576000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.843334e+00</td>\n",
       "      <td>2.495339e+00</td>\n",
       "      <td>3.352078e+00</td>\n",
       "      <td>3.508225e+00</td>\n",
       "      <td>1.343622e+00</td>\n",
       "      <td>1.765426e+00</td>\n",
       "      <td>3.227049e+00</td>\n",
       "      <td>3.421301e+00</td>\n",
       "      <td>1.878128e+00</td>\n",
       "      <td>4.183017e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  5.642000e+04  5.642000e+04  5.642000e+04  5.642000e+04  5.642000e+04   \n",
       "mean  -8.056890e-17  2.219423e-16  3.626734e-16 -1.928524e-15  1.212912e-16   \n",
       "std    1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00   \n",
       "min   -1.507685e+00 -3.142579e+00 -2.886289e+00 -3.037021e-01 -1.488843e+00   \n",
       "25%   -6.695382e-01 -7.581500e-01 -7.917818e-01 -3.037021e-01 -7.313184e-01   \n",
       "50%   -1.806192e-01 -4.126302e-02 -4.579309e-02 -3.037021e-01 -1.361205e-01   \n",
       "75%    1.146447e+00  7.691310e-01  7.862713e-01 -2.181681e-01  5.131863e-01   \n",
       "max    1.705211e+00  2.795116e+00  3.425924e+00  2.909146e+01  2.047937e+01   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  5.642000e+04  5.642000e+04  5.642000e+04  5.642000e+04  5.642000e+04   \n",
       "mean   3.531068e-16 -2.294513e-16 -1.121759e-15 -1.298028e-16  4.271781e-16   \n",
       "std    1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00   \n",
       "min   -2.058376e+00 -1.562005e+00 -2.390483e+00 -1.528270e+00 -1.607592e+00   \n",
       "25%   -7.279240e-01 -9.359957e-01 -7.407036e-01 -8.846016e-01 -9.708734e-01   \n",
       "50%    2.300018e-01  1.073528e-01 -1.407837e-01 -2.637650e-02  9.032404e-02   \n",
       "75%    7.887918e-01  9.420317e-01  5.341262e-01  8.318486e-01  9.392820e-01   \n",
       "max    1.799936e+00  1.568041e+00  6.233365e+00  1.690074e+00  1.576000e+00   \n",
       "\n",
       "       ...            12            13            14            15  \\\n",
       "count  ...  5.642000e+04  5.642000e+04  5.642000e+04  5.642000e+04   \n",
       "mean   ... -1.001975e-16  8.549819e-17  4.202553e-15  2.146056e-14   \n",
       "std    ...  1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00   \n",
       "min    ... -3.558239e+00 -2.455925e+00 -5.317403e+00 -5.486320e+00   \n",
       "25%    ... -5.873737e-01 -7.229829e-01 -6.570142e-01 -6.834078e-01   \n",
       "50%    ...  6.081509e-02  1.970677e-02 -5.717731e-03 -1.391092e-02   \n",
       "75%    ...  7.090039e-01  6.633712e-01  6.600520e-01  6.701403e-01   \n",
       "max    ...  1.843334e+00  2.495339e+00  3.352078e+00  3.508225e+00   \n",
       "\n",
       "                 16            17            18            19            20  \\\n",
       "count  5.642000e+04  5.642000e+04  5.642000e+04  5.642000e+04  5.642000e+04   \n",
       "mean   3.214695e-16 -2.014714e-15  6.301431e-17 -3.205998e-17 -1.324967e-15   \n",
       "std    1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00   \n",
       "min   -1.516445e+00 -1.634357e+00 -2.878373e+00 -2.780718e+00 -5.324452e-01   \n",
       "25%   -1.158937e+00 -8.788497e-01 -7.772553e-01 -7.767640e-01 -5.324452e-01   \n",
       "50%    2.710968e-01  2.544113e-01 -6.165730e-02 -4.539370e-02 -5.324452e-01   \n",
       "75%    9.861134e-01  1.009919e+00  7.757447e-01  7.591136e-01 -5.324452e-01   \n",
       "max    1.343622e+00  1.765426e+00  3.227049e+00  3.421301e+00  1.878128e+00   \n",
       "\n",
       "                 21  \n",
       "count  5.642000e+04  \n",
       "mean  -7.500922e-16  \n",
       "std    1.000009e+00  \n",
       "min   -2.687829e-01  \n",
       "25%   -2.687829e-01  \n",
       "50%   -2.687829e-01  \n",
       "75%   -2.000686e-01  \n",
       "max    4.183017e+01  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training data displayed and normalized\n",
    "inStandardized = scaler.transform(inData)\n",
    "\n",
    "data = pd.DataFrame(inStandardized)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_129\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_385 (Dense)            (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_386 (Dense)            (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_387 (Dense)            (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 113\n",
      "Trainable params: 113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# An example of what a model can look like\n",
    "def initalizeModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(4, input_dim = 8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.01)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "deepLearningModel = initalizeModel()\n",
    "print(deepLearningModel.summary())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\behnk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] batch_size=2, epochs=2 ..........................................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 243s 5ms/step - loss: 0.4659 - accuracy: 0.7961\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 235s 5ms/step - loss: 0.4276 - accuracy: 0.8310\n",
      "11284/11284 [==============================] - 43s 4ms/step\n",
      "[CV] .............. batch_size=2, epochs=2, score=0.834, total= 9.2min\n",
      "[CV] batch_size=2, epochs=2 ..........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  9.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 274s 6ms/step - loss: 0.4638 - accuracy: 0.8001\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 248s 5ms/step - loss: 0.4292 - accuracy: 0.8307\n",
      "11284/11284 [==============================] - 46s 4ms/step\n",
      "[CV] .............. batch_size=2, epochs=2, score=0.833, total=10.0min\n",
      "[CV] batch_size=2, epochs=2 ..........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 19.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 248s 5ms/step - loss: 0.4636 - accuracy: 0.7978\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 264s 6ms/step - loss: 0.4215 - accuracy: 0.8386\n",
      "11284/11284 [==============================] - 44s 4ms/step\n",
      "[CV] .............. batch_size=2, epochs=2, score=0.842, total= 9.8min\n",
      "[CV] batch_size=2, epochs=2 ..........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 28.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 258s 6ms/step - loss: 0.4643 - accuracy: 0.7974\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 250s 6ms/step - loss: 0.4303 - accuracy: 0.8283\n",
      "11284/11284 [==============================] - 46s 4ms/step\n",
      "[CV] .............. batch_size=2, epochs=2, score=0.851, total= 9.7min\n",
      "[CV] batch_size=2, epochs=2 ..........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 38.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 298s 7ms/step - loss: 0.4562 - accuracy: 0.80920s - loss: 0.4563 - accuracy: \n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 254s 6ms/step - loss: 0.4193 - accuracy: 0.8379\n",
      "11284/11284 [==============================] - 44s 4ms/step\n",
      "[CV] .............. batch_size=2, epochs=2, score=0.853, total=10.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 49.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 49.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "56420/56420 [==============================] - 313s 6ms/step - loss: 0.4500 - accuracy: 0.8113\n",
      "Epoch 2/2\n",
      "56420/56420 [==============================] - 327s 6ms/step - loss: 0.4175 - accuracy: 0.8411\n",
      "Best: 0.8425203800201416, using {'batch_size': 2, 'epochs': 2}\n",
      "0.8425203800201416 (0.008076475375438569) with: {'batch_size': 2, 'epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "def initalizeModel():\n",
    "    # Get sequenctial keras model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Layer\n",
    "    model.add(Dense(128, input_dim = 22, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Hidden Layer 1\n",
    "    model.add(Dense(64, input_dim = 128, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #  Hidden Layer 2\n",
    "    model.add(Dense(32, input_dim = 64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(1, input_dim = 32, activation='sigmoid'))\n",
    "    \n",
    "    # Set the learning rate\n",
    "    adam = Adam(lr = 0.0008)\n",
    "    \n",
    "    # Compile the Model\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Defining a random seed\n",
    "randomSeed = 2\n",
    "np.random.seed(randomSeed)\n",
    "\n",
    "model = KerasClassifier(build_fn = initalizeModel, verbose = 1) \n",
    "\n",
    "#define grid search parameters\n",
    "batch_size = [2] # number of steps the moBadels should look at\n",
    "epochs = [2] # how times to run through\n",
    "\n",
    "# make a dictionary of grid seacrh params\n",
    "parameterGrid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "\n",
    "# Build and Fit\n",
    "grid = GridSearchCV(estimator = model, param_grid = parameterGrid, cv = KFold(random_state= randomSeed), refit = True, verbose = 10)\n",
    "\n",
    "grid_results = grid.fit(inStandardized, outData)\n",
    "\n",
    "# Display Results\n",
    "print(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('{0} ({1}) with: {2}'.format(mean, stdev, param))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56420/56420 [==============================] - 212s 4ms/step\n",
      "Actual Data: [0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0]\n",
      "Predictions: [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0]\n",
      "Overall Accuracy Score: 0.8466855724920241\n",
      "\n",
      "               Classification Report\n",
      "=====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91     43993\n",
      "           1       1.00      0.30      0.47     12427\n",
      "\n",
      "    accuracy                           0.85     56420\n",
      "   macro avg       0.92      0.65      0.69     56420\n",
      "weighted avg       0.87      0.85      0.81     56420\n",
      "\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "# Get some detailed reuslts\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "predictions = grid.predict(inStandardized)\n",
    "blankList = []\n",
    "for element in predictions:\n",
    "    for value in element:\n",
    "        blankList.append(value)\n",
    "blankList = np.array(blankList)\n",
    "print(\"Actual Data: {0}\".format(outData[:20]))\n",
    "print(\"Predictions: {0}\".format(blankList[:20]))\n",
    "print(\"Overall Accuracy Score: {0}\".format(accuracy_score(outData, blankList)))\n",
    "print()\n",
    "print(\"               Classification Report\")\n",
    "print(\"=====================================================\")\n",
    "print(classification_report(outData, blankList))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\behnk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] dropout_rate=0.0, learn_rate=0.0001 .............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 325s 7ms/step - loss: 0.3935 - accuracy: 0.8546\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 333s 7ms/step - loss: 0.2750 - accuracy: 0.9304\n",
      "11284/11284 [==============================] - 56s 5ms/step\n",
      "[CV] . dropout_rate=0.0, learn_rate=0.0001, score=0.973, total=12.5min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.0001 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 12.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 315s 7ms/step - loss: 0.3933 - accuracy: 0.8542\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 322s 7ms/step - loss: 0.2592 - accuracy: 0.9364\n",
      "11284/11284 [==============================] - 63s 6ms/step\n",
      "[CV] . dropout_rate=0.0, learn_rate=0.0001, score=0.971, total=12.3min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.0001 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 24.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 329s 7ms/step - loss: 0.4074 - accuracy: 0.8477\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 350s 8ms/step - loss: 0.2899 - accuracy: 0.9339: 15s - loss: 0.2902 - accuracy: 0.933 - ETA: 15s - lo - ETA: 14s - l - ETA: 12s - los - ETA: 5s - loss: 0.2\n",
      "11284/11284 [==============================] - 60s 5ms/step\n",
      "[CV] . dropout_rate=0.0, learn_rate=0.0001, score=0.922, total=13.0min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.0001 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 37.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 335s 7ms/step - loss: 0.4050 - accuracy: 0.8470\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 343s 8ms/step - loss: 0.2818 - accuracy: 0.93159s - loss: 0.2829 - accuracy:  - ETA: 8s - - ETA: 8s - los - ETA: 4s - loss: 0.2822  - - ETA: 0s - loss: 0.2818 - accuracy\n",
      "11284/11284 [==============================] - 63s 6ms/step\n",
      "[CV] . dropout_rate=0.0, learn_rate=0.0001, score=0.951, total=13.2min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.0001 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 51.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 322s 7ms/step - loss: 0.4081 - accuracy: 0.8431\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 317s 7ms/step - loss: 0.2997 - accuracy: 0.9254\n",
      "11284/11284 [==============================] - 59s 5ms/step\n",
      "[CV] . dropout_rate=0.0, learn_rate=0.0001, score=0.939, total=12.3min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.001 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 63.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 371s 8ms/step - loss: 0.3939 - accuracy: 0.8600\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 324s 7ms/step - loss: 0.3390 - accuracy: 0.9061: 51s - loss: 0.3371 - accuracy - ETA: 51s - loss: 0.3371 - accuracy: 0.90 - - ETA: 48s - loss: 0.3369 - accuracy: 0.9 - ETA: 48s - loss: 0.3369 - - ETA: 42s - loss: 0.3372 - - ETA: 41s - loss: 0.3372 - accuracy: 0.906 - ETA: 41s - loss: 0.3372 - accuracy:  - ETA:  - ETA: 33s - loss: 0.3382 - accuracy: 0.9 - ETA: 32s - loss: 0.3382 - accuracy - E - ETA: 25s - loss: 0.3379 - accur - ETA: 24s - loss: 0.3380 - acc - ETA: - ETA: 19s - loss: 0.3379 - accuracy - ETA: 18s - loss: 0.3379 - accuracy: - ETA: 18s - loss: 0.3381 - accu\n",
      "11284/11284 [==============================] - 59s 5ms/step\n",
      "[CV] .. dropout_rate=0.0, learn_rate=0.001, score=0.807, total=13.3min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.001 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 76.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 330s 7ms/step - loss: 0.3869 - accuracy: 0.8612\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 346s 8ms/step - loss: 0.3342 - accuracy: 0.9132\n",
      "11284/11284 [==============================] - 57s 5ms/step\n",
      "[CV] .. dropout_rate=0.0, learn_rate=0.001, score=0.851, total=12.9min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.001 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 89.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 338s 7ms/step - loss: 0.3812 - accuracy: 0.8644\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 357s 8ms/step - loss: 0.3173 - accuracy: 0.9225: 1:00 - loss: \n",
      "11284/11284 [==============================] - 60s 5ms/step\n",
      "[CV] .. dropout_rate=0.0, learn_rate=0.001, score=0.868, total=13.3min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.001 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 102.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 349s 8ms/step - loss: 0.3933 - accuracy: 0.8620\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 366s 8ms/step - loss: 0.3540 - accuracy: 0.8954: 3:32 - ETA: 3:31 - loss: 0.347 - ETA: 3:31 - loss: 0.3463 - accuracy:  - ETA: 3:31 - - ETA: 3:25 - loss: 0.3471 - accuracy:  - ETA: 3:24 - loss\n",
      "11284/11284 [==============================] - 63s 6ms/step\n",
      "[CV] .. dropout_rate=0.0, learn_rate=0.001, score=0.870, total=15.1min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.001 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 118.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 340s 8ms/step - loss: 0.3926 - accuracy: 0.8592\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 356s 8ms/step - loss: 0.3317 - accuracy: 0.9062\n",
      "11284/11284 [==============================] - 63s 6ms/step\n",
      "[CV] .. dropout_rate=0.0, learn_rate=0.001, score=0.859, total=13.5min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.01 ...............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 361s 8ms/step - loss: 0.4831 - accuracy: 0.7763: 1:03 - loss: 0.4836 -  - ETA: 55s - loss: 0.4838 - a - ETA: 55s - loss: 0.4839 - - ETA: 47s - loss: 0.4834 - ETA - ETA: 44s - loss: 0.4836 - accuracy: 0.7 -  - ETA: - ETA: 39s - loss: 0.4838 - a - ETA: 35s - loss: 0.4837 - accuracy: 0.7 - ETA: 35s  - ETA: 34 - E - ETA: 27s  - ETA: 25s - loss: 0.4826 - accuracy: 0.777 - ETA: 25s - loss: 0.4826 - accu - ETA: 24s - loss: 0.4826 - accurac - ETA - ETA: 22s - loss: 0.4826 - accuracy: 0.777 - ETA: 22s - loss: 0.4826 - ac - ETA: 21s - loss - ETA: 17s - loss: 0.4821 - accura - ETA: 17s - loss: 0.4821 - accuracy: 0.777 - ETA: 17s -  - ETA: 15s -  - ETA: 11s - loss: 0.4822 - ETA: 4s - loss: 0.4827 - accuracy - E - ETA: 2s - loss:\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 382s 8ms/step - loss: 0.4697 - accuracy: 0.7801: 57s - loss: 0.4709 - accuracy: 0. - ETA: 57s - loss: 0.4709 - accuracy: - ETA: 56s - loss: 0.4711 - accuracy: 0 - ETA: 56s - loss: 0.4 - ETA: 46s -  - ETA: 38s - loss: 0.4709 - accuracy: 0.7 - ETA: 38s - loss: 0.4708 - accuracy: 0. - ETA: 38s  - ETA: 36s - loss: 0.4712 - accuracy: 0 - ETA: 36s - loss: 0.4712 - accuracy: 0.7 - ETA: 36s - loss: 0.4711 - accuracy: 0.779 - ETA: 36s -  - ETA: 35s - loss: 0.4 - ETA: 34s - loss: 0.4712 - accuracy: 0. - ETA: 33s -  - ETA: 32s - loss: 0.4709 - accuracy: 0. - ETA: 32s - loss: 0.4709 - accuracy: 0.779 - ETA: 32s - loss: 0.4709 - - ETA: 31s - loss: 0.4708 - accuracy: 0.77 - ETA: 31s - loss: 0.4708 - accuracy: 0. - ETA: 31s - loss: 0.4707 - accuracy: 0.779 - ETA: 31s - los - ETA: 29s - loss: 0.4708 - accuracy - ETA: 29s - loss: 0.4709 - a - ETA: 28s - loss: 0.4708 - accuracy: 0 - ETA: 28s - loss: 0.4709 - accuracy: 0.77 - ETA: 26s - loss: 0.4707 - accuracy: 0.779 - ETA: 26s - loss: 0.4707 - accuracy: 0. - ETA: 25s - loss: 0.4708 - accuracy: 0.779 - ETA: 25s - loss:  - ETA: 24s - loss: 0.4708 - acc - ETA: 24s - loss: 0.4707  - ETA: 18s - loss: 0.4705 - - ETA: 17s - loss: 0.4703 - accur - ETA: 17s - loss: 0.4702 - accura - ETA: 16s - loss: 0.4701 - accuracy: 0.780 - ETA: 16s - ETA: 1s\n",
      "11284/11284 [==============================] - 62s 6ms/step\n",
      "[CV] ... dropout_rate=0.0, learn_rate=0.01, score=0.788, total=14.3min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.01 ...............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 369s 8ms/step - loss: 0.4797 - accuracy: 0.7816\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 368s 8ms/step - loss: 0.4787 - accuracy: 0.7789: 45s - loss: 0.4799 - accuracy: 0.778 - ETA: 45s - loss: 0 - ETA: 44s - loss: 0.4799 - accuracy: 0. - ETA: 44s - loss: 0.4798 - accuracy: 0.778 - ETA: 44s - loss: 0.4798 - accuracy - ETA: 43s - loss: 0.4799 -  - ETA: 41s - loss: 0.4796 - accuracy: 0.778 - ETA: 41s - - ETA: 39s - loss: 0.4796 - accuracy: 0.778 - ETA: 39s - loss: 0.4796 - accuracy: 0.778 - ETA: 39s - loss: 0.4796 - ac - ETA: 38s - loss: 0.4794 - accuracy: 0.778 - ETA: 38s - los - ETA: 37s - loss: 0.4796 - accu - ETA: 37s - loss: 0.4795 - accuracy: 0. - ETA: 36s - loss: 0.4796 - acc - ETA: 36s - loss: 0.4794 - accuracy: 0.778 - ETA: 36s - loss: 0.4793 - accuracy: 0.77 - ETA: 36s - loss: 0.4794 - accuracy: 0.778 - ETA: 36s - loss: 0.4793 - accurac - ETA: 35s - loss: 0.4794 - accuracy: - ETA: 33s - loss: 0.4791 - accuracy: - ETA: 33s - loss: 0.4791 - accuracy: 0.779 - ETA: 32s - loss: 0.4791 - accur - ETA: 32s - loss: 0.4790 - - ETA: 31s - loss: 0.4 - ETA: 30s - loss: 0.4788 - accuracy: 0 - ETA: 30s - loss: 0.4789 - ac - ETA: 29s - loss: 0.4789 - accur - ETA: 29s - loss: 0.4790 - accura - ETA: 28s - loss: 0.4789 - accuracy: 0. - ETA: 28s - loss: 0.4789 - a - ETA:  - ETA: 26s - loss: 0.4791 - accuracy: 0.778 - ETA: 26s - loss: 0.4791 - accu - ETA: 26s - loss: 0.4792 - accuracy: 0 - ETA: 25s - loss: 0.4792 - accuracy: 0. - ETA: 25s - loss: 0.4 - ETA: 24s - loss: 0.4 - ETA: 23s - loss: 0.4790 - accuracy: 0.778 - ETA: 23s - loss: 0.4790 - accuracy: 0. - ETA: 23s - loss: 0.4791 - accuracy: 0.778 - ETA: 23s - loss: 0.4791 - accurac - ETA: 22s - loss: 0 - ETA: 21s - loss: 0.4792 - accurac - ETA: 21s - loss: 0.4791 - a - ETA: 20s - loss: 0.4 - ETA: 19s - loss: 0.4794 - accuracy:  - ETA: 19s - l - ETA: 13s - loss: 0. - ETA: 12s - loss: 0.4791 - accuracy: 0.778 - ETA: 12s - loss: 0.4791 -  - ETA: 11s - loss: 0.4792 - accuracy: 0.778 - ETA: 11s -  - ETA: 10s - loss: 0.4794 - accur - ETA:  - ETA: 3s - loss: 0.4791 - accura\n",
      "11284/11284 [==============================] - 70s 6ms/step\n",
      "[CV] ... dropout_rate=0.0, learn_rate=0.01, score=0.781, total=14.3min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.01 ...............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 378s 8ms/step - loss: 0.4826 - accuracy: 0.7782\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 374s 8ms/step - loss: 0.4800 - accuracy: 0.7782: 39s - lo - ETA: 34s - loss: 0.4806 - a - E - ETA: 26s - loss: 0.4796 - accuracy: 0.778 - ETA: 26s - loss: 0.4796 - accuracy: 0.778 - ETA: 26s - loss: 0.4797 - accuracy: 0. - ETA: 26s - loss: 0.4797 - accuracy:  - ETA: 26s -  - ETA: 24s - loss: 0.4 - ETA: 23s - loss: 0. - ETA: 22s - loss: 0.47 - ETA: 21s - loss: 0.4795 - accurac - ETA: 21s - loss: 0.4794 - accuracy: 0.77 - ETA: 21s - loss: 0.47 - ETA: 18s - loss: 0.4793 - accuracy: 0.778 - ETA: 18s - los - ETA: 16s - loss: 0.4793 - accu - ETA: 16s - loss: 0.4793 - a - ETA: 15s - loss: 0.4793 - accuracy: - ETA: 15s - loss: 0.4794 - accuracy:  - ETA: 14s - loss: 0.4793 - a - ETA: 14s - loss: 0.4792 - accur - ETA: 13s - loss: 0.4792 - acc - ETA: 12s - loss - ETA: 11s - loss: 0.4792 - a - ETA: 11s - loss: 0.4791 - accu - ETA: 10s -  - ETA:  - ETA: 5s - loss: 0.4 - ETA: 4s - loss: 0.4 - ETA: 4s - loss: 0.4 - ETA: 3s - los - ETA - ETA: 1s - loss: 0.479 - ETA: 0s - loss: 0.4798 - accuracy: 0.77 - ETA: 0s - loss: 0.4798 - accuracy: 0.77 - ETA: 0s - loss: 0.4798 - ac - ETA: 0s - loss: 0.4799 - accura\n",
      "11284/11284 [==============================] - 76s 7ms/step\n",
      "[CV] ... dropout_rate=0.0, learn_rate=0.01, score=0.801, total=14.7min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.01 ...............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 361s 8ms/step - loss: 0.4866 - accuracy: 0.7772\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 354s 8ms/step - loss: 0.4814 - accuracy: 0.77910s - loss: 0.4814 - accuracy: 0.\n",
      "11284/11284 [==============================] - 83s 7ms/step\n",
      "[CV] ... dropout_rate=0.0, learn_rate=0.01, score=0.795, total=14.2min\n",
      "[CV] dropout_rate=0.0, learn_rate=0.01 ...............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 417s 9ms/step - loss: 0.4759 - accuracy: 0.7810: 38s - los - ETA: 36s - loss: 0.4750 - accura - ETA: 36s  - ETA: 24s - loss: 0.4761  - ETA: 23s - loss: 0.4760 - ac - ETA: 2 - ETA: 20s - loss: 0.4759  - ET - ETA: 1s - - ETA: 0s - loss: 0.4759 - accuracy: 0.78\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45136/45136 [==============================] - 437s 10ms/step - loss: 0.4716 - accuracy: 0.7850 1 - ETA: 58s - loss: 0 - ETA: 5 - ETA: 50s - loss: 0.4714 - - ETA: 49s - loss: 0.4714 - accuracy: 0 - E - ETA: 47s - loss: 0.4715 -  - ETA: 46s - loss: 0.4717 - accuracy: 0.784 - ETA: 46s - loss:  - ETA: 45s - loss: 0.4714 - ETA: 44s - loss: 0.4717 - acc - ETA: 42s - loss: 0.4719 - accurac - ETA: 42s - loss: 0.4719 - a - - ETA: 37s - loss: 0.4720  - ETA: 37s - loss: 0.4721 - accuracy: 0. - ETA: 36s - loss: 0.4721 - accuracy: 0.784 - ETA: 36s - loss: 0.4720 - accurac - ETA: 36s - loss: 0.4720 - accuracy - ETA: 35s -  - ETA: 34s - loss: 0.4721 - accuracy: 0.78 - ETA: 34s - loss: 0.4722 - accu - ETA: 33s - loss: 0.4722 - accuracy: 0.7 -  - ETA: 31s - loss: 0.4721 - accuracy: 0.78 - ETA: 31s - loss: 0.4721 - accurac - ETA: 31s - loss: 0.47 - ETA: 30s - loss: 0.4722 - accuracy: 0 - ETA: 30s - loss: 0.4723 -  - ETA: 29s - loss:  - ETA: 28s - lo - ETA: 27s - loss: 0 - ETA: 26s - loss: 0.4720 - accuracy: 0.7 - ETA: 26s - loss: 0.4721 - accuracy:  - ETA: 25s - - ETA: 24s - loss: 0.4721 - accuracy: 0. - ETA: 23s - loss: 0.4721 - accuracy: 0.78 - ETA: 23s - los - ETA: 22s - loss: 0.4720 - accuracy: 0.7 - ETA: 22s - loss: 0.472 - ETA: 0s - loss: 0.4718 - accura - ETA: 0s - loss: 0.4716 - \n",
      "11284/11284 [==============================] - 82s 7ms/step\n",
      "[CV] ... dropout_rate=0.0, learn_rate=0.01, score=0.792, total=16.5min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.0001 ............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 416s 9ms/step - loss: 0.4671 - accuracy: 0.7960\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 394s 9ms/step - loss: 0.3641 - accuracy: 0.8807\n",
      "11284/11284 [==============================] - 64s 6ms/step\n",
      "[CV]  dropout_rate=0.25, learn_rate=0.0001, score=0.902, total=15.4min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.0001 ............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 460s 10ms/step - loss: 0.4773 - accuracy: 0.7888\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 416s 9ms/step - loss: 0.3827 - accuracy: 0.8684\n",
      "11284/11284 [==============================] - 69s 6ms/step\n",
      "[CV]  dropout_rate=0.25, learn_rate=0.0001, score=0.869, total=16.6min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.0001 ............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 437s 10ms/step - loss: 0.4674 - accuracy: 0.7985\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 433s 10ms/step - loss: 0.3662 - accuracy: 0.8805\n",
      "11284/11284 [==============================] - 77s 7ms/step\n",
      "[CV]  dropout_rate=0.25, learn_rate=0.0001, score=0.870, total=16.6min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.0001 ............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 430s 10ms/step - loss: 0.4808 - accuracy: 0.7881 1:00 - loss: 0.49 - ETA:\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 448s 10ms/step - loss: 0.3686 - accuracy: 0.8781\n",
      "11284/11284 [==============================] - 81s 7ms/step\n",
      "[CV]  dropout_rate=0.25, learn_rate=0.0001, score=0.881, total=16.9min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.0001 ............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 472s 10ms/step - loss: 0.4789 - accuracy: 0.7872 58s - loss: 0.4891 - accuracy: 0.77 - ETA: - ETA: 57s - loss: 0.4884 - accurac - ETA: 57s - loss: 0.4883 - accuracy: 0.78 - ETA: 56s - loss:  - ETA: 55s - loss: 0.4880  - ETA: -  - ETA: 50s - loss: 0.4869  -  - ETA: 33 - ETA: 31s - loss: 0.4842 - ac - ETA: 31s - loss: 0.4841 - accuracy:  - ETA: 31s - loss: 0.4840 - accur - ETA: 30s - loss: 0.4838 - acc - ETA: 29s - loss: 0.4836 - accu - ETA: 26s - loss: 0.4832 - accuracy: 0.784 - ETA: 26s - loss: 0.4831 - accuracy: 0.7 - ETA: 25s - loss: 0.4832 - accuracy:  - ETA: 15s - loss: 0.4815 - accuracy: 0 - ETA: 15s - loss:  -  - ETA: 10s - \n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 503s 11ms/step - loss: 0.3791 - accuracy: 0.8710 1:22 - loss: 0.3867 - accuracy: 0.86 - ETA: 1:21 - loss: 0.3866 - a - ETA: 59s - loss: 0.3856 -  - ETA: 58s - loss: 0.3858 - accuracy: 0 - E - ETA: 55s - loss: 0 - ETA: 54s - loss: 0.3854 - accura - ETA: 53s - loss: 0.385 - ETA: 52s - loss: 0.3851  - ETA: 51s - loss: 0.3850 - accuracy: 0. - ETA: 51s - loss: 0.3850 - accuracy: 0.86 - ETA: 50s - loss: 0.3851 - accur - ETA: 50s - loss: 0.3851 - ac - ETA: 49s - lo - ETA: 48s - los - ETA: 46s - loss: 0.3848 -  - ETA: 45s - loss:  - ETA: 43s - loss: 0.3842 - accuracy: 0. - ETA: 43s - loss: 0.3841 - accuracy: 0.868 - ETA:  - ETA:  - ETA: 35s - loss: 0.3831 - accurac - ETA: 35s - loss: 0.3830 - accuracy - ETA: 34s - loss: 0.3830 - accuracy: 0. - ETA: 34s - loss: 0.383 - ETA - ETA: 30s - loss: 0. - ETA: 29s - loss - ETA: 27s - loss: 0.3822 - accu - ETA: 26s - loss: 0.3822 -  - ETA: 25s - loss: 0.3822 - accuracy: 0 - ETA: 25s - los - ETA: 23s - loss: 0.3818 - accuracy: 0 - ETA: 23s - loss: 0.3817 - accur - ETA: 22 - ETA: 20s - loss: 0.3811 - accuracy: 0.869 - ETA: 20s - loss: 0.3811 - accu - ETA: 19s - loss: 0.3808 - accura - ETA: 19s - loss: 0. - ETA: 15s -  - ETA: 10s - ETA: 9s - loss: 0.3801 - accuracy - E - ETA - ETA: 3s - loss: 0.3797 - accuracy:  - ETA: 3s - loss: 0.3 - ETA: 2s - l - ETA: 0s - loss: 0.3791 - accuracy: 0.\n",
      "11284/11284 [==============================] - 87s 8ms/step\n",
      "[CV]  dropout_rate=0.25, learn_rate=0.0001, score=0.895, total=18.7min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.001 .............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 425s 9ms/step - loss: 0.4242 - accuracy: 0.8313\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 385s 9ms/step - loss: 0.3807 - accuracy: 0.8724\n",
      "11284/11284 [==============================] - 69s 6ms/step\n",
      "[CV] . dropout_rate=0.25, learn_rate=0.001, score=0.852, total=15.7min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.001 .............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 420s 9ms/step - loss: 0.4269 - accuracy: 0.8296\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 389s 9ms/step - loss: 0.3768 - accuracy: 0.8751\n",
      "11284/11284 [==============================] - 69s 6ms/step\n",
      "[CV] . dropout_rate=0.25, learn_rate=0.001, score=0.851, total=15.5min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.001 .............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 421s 9ms/step - loss: 0.4253 - accuracy: 0.8346\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 391s 9ms/step - loss: 0.3765 - accuracy: 0.8765\n",
      "11284/11284 [==============================] - 70s 6ms/step\n",
      "[CV] . dropout_rate=0.25, learn_rate=0.001, score=0.861, total=15.6min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.001 .............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 428s 9ms/step - loss: 0.4224 - accuracy: 0.83403s - loss: 0.4227 - accuracy: 0.83 - ETA: 3s - loss: 0.4227 - accuracy:  - ETA: 3s - loss: 0.4226  - ETA:  - ETA: 1s - loss: 0.4225 - accuracy:  - ETA: 1s - loss: 0.4225 - accu - ETA: 1s - loss: 0.4226 - accu - ETA\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 403s 9ms/step - loss: 0.3722 - accuracy: 0.8833\n",
      "11284/11284 [==============================] - 71s 6ms/step\n",
      "[CV] . dropout_rate=0.25, learn_rate=0.001, score=0.856, total=15.9min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.001 .............................\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45136/45136 [==============================] - 415s 9ms/step - loss: 0.4220 - accuracy: 0.8357\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 393s 9ms/step - loss: 0.3777 - accuracy: 0.8733\n",
      "11284/11284 [==============================] - 74s 7ms/step\n",
      "[CV] . dropout_rate=0.25, learn_rate=0.001, score=0.873, total=15.6min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.01 ..............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 396s 9ms/step - loss: 0.4830 - accuracy: 0.7801\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 426s 9ms/step - loss: 0.4799 - accuracy: 0.7791\n",
      "11284/11284 [==============================] - 90s 8ms/step\n",
      "[CV] .. dropout_rate=0.25, learn_rate=0.01, score=0.792, total=16.3min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.01 ..............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 408s 9ms/step - loss: 0.4827 - accuracy: 0.7802\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 411s 9ms/step - loss: 0.4887 - accuracy: 0.77740s - loss: 0.4887 - accuracy: 0. - ETA: 0s - loss: 0.4887 - accu\n",
      "11284/11284 [==============================] - 75s 7ms/step\n",
      "[CV] .. dropout_rate=0.25, learn_rate=0.01, score=0.779, total=15.8min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.01 ..............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 410s 9ms/step - loss: 0.4871 - accuracy: 0.7757\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 418s 9ms/step - loss: 0.4833 - accuracy: 0.7788: 1:05  - ETA: 41s - loss: 0.4825 - accuracy: 0 - ETA: 41s - loss: 0.4824 - accuracy - ETA: 41s - loss: 0.4823 - accuracy: 0.778 - ETA: 41s - loss: 0.4822 - accurac\n",
      "11284/11284 [==============================] - 71s 6ms/step\n",
      "[CV] .. dropout_rate=0.25, learn_rate=0.01, score=0.790, total=15.9min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.01 ..............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 412s 9ms/step - loss: 0.4862 - accuracy: 0.7773\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 423s 9ms/step - loss: 0.4947 - accuracy: 0.7788\n",
      "11284/11284 [==============================] - 72s 6ms/step\n",
      "[CV] .. dropout_rate=0.25, learn_rate=0.01, score=0.797, total=16.0min\n",
      "[CV] dropout_rate=0.25, learn_rate=0.01 ..............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 413s 9ms/step - loss: 0.4844 - accuracy: 0.7794\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 424s 9ms/step - loss: 0.4716 - accuracy: 0.7790\n",
      "11284/11284 [==============================] - 72s 6ms/step\n",
      "[CV] .. dropout_rate=0.25, learn_rate=0.01, score=0.839, total=16.4min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.0001 .............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 419s 9ms/step - loss: 0.5335 - accuracy: 0.7468\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 429s 10ms/step - loss: 0.4258 - accuracy: 0.8314\n",
      "11284/11284 [==============================] - 73s 6ms/step\n",
      "[CV] . dropout_rate=0.5, learn_rate=0.0001, score=0.875, total=16.3min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.0001 .............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 427s 9ms/step - loss: 0.5597 - accuracy: 0.7427\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 435s 10ms/step - loss: 0.4301 - accuracy: 0.8288\n",
      "11284/11284 [==============================] - 74s 7ms/step\n",
      "[CV] . dropout_rate=0.5, learn_rate=0.0001, score=0.861, total=16.5min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.0001 .............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 451s 10ms/step - loss: 0.5566 - accuracy: 0.7410 1:23 - ETA: 1:20 - loss: 0.5 - ETA: 1:19 - loss: 0 - ETA: 1:19 - loss: 0.5755 - accuracy - ETA: 1:19 - loss: 0 - - ETA: 1:02 - loss: 0.570 - ETA: 1:01 - loss: 0.5703 - accuracy - ETA: 1:01 - loss: 0.5702 - accu - ETA: 1:01 - loss: 0.5 - ETA:  - ETA: 58s - loss: 0.5694 - accuracy - ETA: 57s - loss: - ETA: 56s - loss: 0.5689 - accur - ETA: 55s - loss - ETA: 54s - loss: 0.5686 - accuracy - ETA: 54s - loss: 0.5684 - accuracy: 0.7 - ETA: 54s - loss: 0.5683 - a - ETA: 53s - loss: 0.5683 - accuracy - ETA: 52s - l - ETA: 51s - loss: 0.5679 - accurac - ETA: 51s - loss: 0.5678 - accuracy: 0. - ETA: 50s - loss: 0.5677 - accura - ETA: 50s - loss: 0.5675 - ETA: 49s - loss: 0.5673 - accuracy: 0. - ETA: 49s - l - ETA: 47s - loss: 0.5668 - accu - ETA: 47s - loss: 0.5666 - accu - ETA: 46s - loss: 0.5666 - accuracy: 0. - ETA: 46s - loss - ETA: 45s - loss: 0.5663 - accuracy: 0 -  - ETA: 38s - loss: 0.5650 - accurac - ETA - ETA: 33s - loss: 0.5636 - a - ETA: 33s - loss:  - ETA: 31s - loss: 0.5632 - accura - ETA: 31s - - ETA: 29s - loss: 0.5631 - accuracy: 0.7 - ETA: 29s  - ETA: 28s - loss: 0.5631 -  - ETA: 27s - loss: 0.563 - ETA: 26s  - ETA: 24s - loss: 0.5 - ETA: 19s -  - ETA - ETA:  - ETA\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 413s 9ms/step - loss: 0.4267 - accuracy: 0.8299\n",
      "11284/11284 [==============================] - 76s 7ms/step\n",
      "[CV] . dropout_rate=0.5, learn_rate=0.0001, score=0.866, total=16.6min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.0001 .............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 446s 10ms/step - loss: 0.5257 - accuracy: 0.7537\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 417s 9ms/step - loss: 0.4231 - accuracy: 0.8347\n",
      "11284/11284 [==============================] - 75s 7ms/step\n",
      "[CV] . dropout_rate=0.5, learn_rate=0.0001, score=0.892, total=16.5min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.0001 .............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 455s 10ms/step - loss: 0.5118 - accuracy: 0.7607\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 426s 9ms/step - loss: 0.4143 - accuracy: 0.8421\n",
      "11284/11284 [==============================] - 77s 7ms/step\n",
      "[CV] . dropout_rate=0.5, learn_rate=0.0001, score=0.873, total=16.9min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.001 ..............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 460s 10ms/step - loss: 0.4591 - accuracy: 0.7985\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 427s 9ms/step - loss: 0.4316 - accuracy: 0.8277\n",
      "11284/11284 [==============================] - 84s 7ms/step\n",
      "[CV] .. dropout_rate=0.5, learn_rate=0.001, score=0.841, total=17.1min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.001 ..............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 433s 10ms/step - loss: 0.4661 - accuracy: 0.7955\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 436s 10ms/step - loss: 0.4235 - accuracy: 0.8322 40s - loss: 0.4242 - accur - ETA: 36s - loss: 0.4237 - ETA: 35s - loss: 0.4238 - accuracy: 0. - ETA: 35s - loss: 0.4237 - accura - ETA: 34s - loss: - ETA: 33s - loss: 0.4237 - a - ETA: 33s - loss: 0.4236 - acc - ETA: 32s - loss: 0.4236 - accuracy: - ETA: 32s - loss: 0.4236 - accuracy: - ETA: 31s - loss: 0.4235 - accu - ETA: 31s -  - E - ETA: 26s - loss: 0.4235  - ETA: 23s - loss: 0.4239 - accuracy: 0.83 - ETA: 23s - loss: - ETA: 21s - loss: 0.4238 - accura - ETA: 21s - loss: 0.4237 - accuracy: 0.83 - ETA: 21s - los - ETA: 20s - loss: 0.4236 - - ETA: 19s - loss: 0.4239 - accuracy: 0 - ETA: 19s - loss: 0.4238 - accuracy: 0.8 - ETA: 18s - loss: 0.4239 - accuracy: 0 - ETA: 18s - loss: 0.4238 - accuracy - ETA: 18s - loss: 0 - ETA: 17s - - ETA: 8s - loss: 0.4235 - accuracy: 0. - ETA: 8s - loss: 0.4235 - ac - ETA: 6s - loss: 0.4230 - accuracy: 0. - ETA: 5s - loss: 0.4234 -  - - ETA: 4s - los - ETA: 3s - loss: 0.423 - ETA: 3s - - ETA: 2s - loss: 0.4235 - accuracy:  - ETA:  - ETA: 1s - loss: 0.4234 -  -\n",
      "11284/11284 [==============================] - 83s 7ms/step\n",
      "[CV] .. dropout_rate=0.5, learn_rate=0.001, score=0.831, total=17.0min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.001 ..............................\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45136/45136 [==============================] - 450s 10ms/step - loss: 0.4616 - accuracy: 0.7971s - los - ETA: 0s - loss: 0.4616 - accuracy\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 451s 10ms/step - loss: 0.4302 - accuracy: 0.8284\n",
      "11284/11284 [==============================] - 79s 7ms/step\n",
      "[CV] .. dropout_rate=0.5, learn_rate=0.001, score=0.854, total=20.1min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.001 ..............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 458s 10ms/step - loss: 0.4667 - accuracy: 0.7940 50s - loss: 0.4716 - accuracy:  - ETA: 49s - loss: 0.4716 - accuracy: 0 - ETA: 49s - loss: 0. - ETA: 48s - loss: 0.4716 - accur - ETA: 42s - loss: 0.4708 -  - ETA: 41s - loss: 0.4707 - accuracy: 0.79 - ETA: 41s - loss: 0.4707 - accuracy - ETA: 40s - los - ETA: 39s \n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 435s 10ms/step - loss: 0.4359 - accuracy: 0.8185\n",
      "11284/11284 [==============================] - 77s 7ms/step\n",
      "[CV] .. dropout_rate=0.5, learn_rate=0.001, score=0.864, total=17.2min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.001 ..............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 457s 10ms/step - loss: 0.4591 - accuracy: 0.7985\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 427s 9ms/step - loss: 0.4263 - accuracy: 0.8289\n",
      "11284/11284 [==============================] - 80s 7ms/step\n",
      "[CV] .. dropout_rate=0.5, learn_rate=0.001, score=0.866, total=17.1min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.01 ...............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 513s 11ms/step - loss: 0.5084 - accuracy: 0.7734 1:07 - loss: 0.5089 - accuracy: 0. - ETA: 1:06 - loss: 0 - ETA: 1:06 - loss: 0.5089 - accuracy - ETA: 1:05 - loss: 0.5088 - accuracy: 0.77 - ETA: 1:05 - loss: 0.5088 -  - ETA: 1:05 - los - ETA: 1:02 - loss: 0.5090  - ETA: 1:01 - loss: 0.50 - ETA: 53s - loss: 0.5089 - accuracy - ETA: 52s - loss: 0.5088 - accuracy: 0.772 - ETA: 52s - loss: 0.5088 - accuracy: 0. - ETA: 52s  - ETA: 38s - loss: 0.5090 - accuracy: 0.772 - ETA: 38s - loss: 0.5090 - accuracy: 0 - ETA: 38s - loss: 0.5090 - accuracy: 0.772 - ETA: 37s - l - ET - ETA: 33s - loss: 0.5085 - accuracy: 0.772 - ETA: 33s - loss: 0.5086 - accuracy: 0. - ETA: 32s - loss: 0.5086 - accuracy:  - ETA: 32s - loss: 0.5086 - accuracy: 0.772 - ETA: 32s - loss: 0.5086 - ac - ETA: 31s - loss: 0.5086 - accura - ETA: 31s - loss: 0.5088 - a - ETA: 30s - loss: 0.5087 - accuracy: 0.772 - ETA: 27s - loss - ETA: 25s - loss: 0.5088 - accuracy: 0 - ETA: 25s - loss: 0.5089 - accuracy: 0.772 - ETA: 25s - loss: 0.5089 - accura - ETA: 0s - loss: 0.5085 - accu - ETA: 0s - loss: 0.5085 - accuracy: 0.\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 450s 10ms/step - loss: 0.5089 - accuracy: 0.7742\n",
      "11284/11284 [==============================] - 84s 7ms/step\n",
      "[CV] ... dropout_rate=0.5, learn_rate=0.01, score=0.786, total=18.5min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.01 ...............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 495s 11ms/step - loss: 0.5011 - accuracy: 0.7753 2:08 - loss: 0.4994 - accuracy:  - ETA: 2:07 - l - ETA: 2:07 - loss: 0.4995  - ETA: 2:06 - loss: 0.4996 - accuracy:  - ETA: 2:06 - loss: 0.4 - ETA: 1:37 - ETA: 1:36 - ETA: 1:35 - ETA: 1:34 - l - ETA: 1:16 - los - ETA: 1:15 - los - ETA: 1:14 - ETA: 1:12 - loss: 0.5000 - accura - ETA: 1: - ETA: 1:11 - loss: 0.5001 - accura - ETA: 1:11 - l - ETA: 1:08 - loss: 0.5001 - accuracy: 0.77 - ETA: 1:08 - loss: - ETA: 1:07 - loss: 0.5 - ETA: 1:07 - loss: - ETA: 1:02 - l - ETA: 1:01 - los - ETA: 49s - loss: 0.50 - ETA: 45s - loss: 0.5011 - accuracy: 0. - ETA: 4 - ETA: 40s - loss: 0.5013 - ETA: 39s - loss: 0.5013 - ETA: 38s - loss: - ETA: 37s - loss: 0.5012 - accuracy: 0. - ETA: 37s - loss: 0.5011 - accuracy: 0.774 - ETA: 37s - loss: 0.5011 - accuracy: 0.77 - ETA: 36s - loss: 0.50 - ETA: 35s - loss: 0.5011 - accuracy - ETA: 35s - loss: 0.5013 - accura - ETA: 35s - loss: 0.501 - ET - ETA: 31s - loss: 0.5014 - accuracy: 0.7 - ETA: 31s - loss: 0.5015 - accurac - ETA: 31s - loss: 0.5014 - accu - ETA: 30s - loss: 0.5012 - accuracy: 0 - ETA: 30s - lo  - ETA: 27s - loss: 0.5009 - a - ETA: 26s - loss: 0.5009 -  - ETA: 25s - loss: 0. - ETA: 24s - loss: 0.5010 - ac - ETA: 23s - loss: 0.5009 - - ETA: - ETA: 21s - loss: 0.5009 - accuracy:  - ETA: 20s - loss: 0.5009 -  - ETA: 20s - loss: 0.5009 - accurac - ETA: 19s - loss: 0.5010 - ETA: 18s - loss: 0.5010 - accuracy: 0 - ETA - ETA: 16s - loss: 0.5011 - accuracy: 0.77   - ETA: 11s - loss: 0.5009 - accuracy:  - ETA: 11s - loss: 0.5009 - accura - ETA: 11s - loss: 0.5009 - accuracy: 0. - ETA: 10s - loss: 0.5010 - accu - ETA: 10s - loss: 0.5009 - accuracy: 0 - ETA: 9s - lo - ETA: 9s - loss: 0.5010  - ETA: 8s - loss: 0.5010 - ac - ETA: 8s - loss: 0.5009 - accu - ETA: 4s - loss: 0 - ETA\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45136/45136 [==============================] - 585s 13ms/step - loss: 0.5006 - accuracy: 0.7761- ETA: 5:53 - loss: 0.4995 - accuracy:  - ETA: 5:46 - loss: - ETA: 5:46 - loss: 0.5002 -  - ETA: 5:46 - loss: 0.5007  - - ETA: 5:41 - loss: 0.5006  - ETA: 5:40 - loss: 0.5004 - ac - ETA: 4:24 - loss: 0.4990 - ac - ETA: 4:22 - loss: 0.4990  - ETA: 4:14 - loss: 0.4988 - accura - ETA: 4:14 - loss: 0.4988 - accura - - ETA: 4:11 - loss: 0.4990 - accura - ETA: 4: - E - ETA: 4:06 - ETA - ETA: 4:05 - loss: 0.4994 - accu - ETA: 4:05 - loss: 0.4994 - ac - ETA: 4:05 - loss: 0.4993 - accuracy: 0. - ETA: 4:05 - loss: 0.4992 - accu - ETA: 4:04 - l - ETA: 3:56 - loss: 0 - ETA: 3:56 - loss: 0.4997 - accuracy:  - ETA - ETA: 3:55 - loss: 0.499 - ETA: 3:54 - loss: - ETA: 3:54 - loss: 0.4993 - accuracy: 0. - ETA: 3:54 - loss: 0.4992 - accu - ETA: 3:53 - loss: 0 - ETA: 3:53 - ETA: 3:50 - loss: 0.4996 - ac - ETA: 3:50 - loss: 0.4996 - accuracy: 0.77 - ETA: 3:50 - loss: 0.4996  - ETA: 3:38 - loss: - ETA: 3:35 - ETA: 3:34 - l - ETA: 3:23 - loss: 0.4985 - accuracy: 0.77 - ETA: 3:23 - loss: 0.4985 - accuracy - ETA: 3:23 - loss: - ETA: 3:22 - loss: 0.4986 - accu - E - ETA:  - ETA: 3:19 - loss: 0.4988 - accuracy - ETA: 3:18 - loss: 0.4988 - ac - ETA: 3:18 - loss: 0.4986 -  - ETA: 3:18 - loss: 0.4986 - accuracy - - ETA: 3:17 - loss: 0 - ETA: 3:16 - loss: 0.498 - ETA: 3:15 - loss: 0.4985 - accura - ETA: 3:15 - loss: 0.4984 -  - - ETA: 2:28 - loss: 0.4984  - ETA: 2:28 - loss: - ETA: 2:27 - loss: 0.4983 -  - ETA: 2:27 - loss: 0.4983 - accuracy: 0.77 - ETA: 2:27 - loss: 0.4983 - accura - ETA: 2:26 - - ETA: 2:24 - loss: 0.4979 - ac - ETA: 2:23 - - ETA: 2:21 - loss: 0.4 - ETA: 2:16 - l - ETA: 2:15 - loss: 0.4979 -  - ETA: 2:14 - loss: 0.4978 - accuracy:  - ETA:  - ETA: 2:11 - los - ETA: 2:10 - loss: 0.498 - ETA: 2:02 - loss: 0.4991 - accura - ETA: 2:01 - loss: 0.499 - ETA: 2: - ETA: 1:59 - los - ETA: 1:58 - loss: 0.4987  - ETA: 1:58 - loss: 0.4988 -  - ETA: 1:57 - loss: 0.4990 - accuracy: 0.77 - ETA: 1:57 - loss: - ETA: 1:56 - loss: 0.4989  - ETA: 1:56 - loss: 0.4987 - accuracy - ETA: 1:55 - loss: - ETA: 1:55 - loss: 0.498 - ETA: 1:18 - loss: 0.4 - ETA: 1:00 -  - ETA: 59s - loss: 0.4997 - accuracy: 0. - ETA: 59s - loss: 0.4996 - accuracy: 0.776 - ETA: 59s - l - ETA: 58s - loss: 0.4996 - accuracy: 0. - ETA: 57s - loss: 0.4996 - accuracy:  - ETA: 57s - loss: 0.4996 - accuracy: 0 - ETA: 57s - loss: 0.4998 - accu - ETA: 56s - loss: 0.4998 - accuracy: 0.776 - ETA: 56s - loss: 0.4999 - accuracy - ETA: 56s - loss - ETA: 54s - loss: 0.5 - ETA: 52s - lo - ETA: 50s - loss: 0.5002 - accurac - ETA: 50s - loss: 0.5003 - accuracy: 0.775 - ETA:  - ETA: 45s - loss: 0.5009 - accuracy: 0.7 - ETA: 45s - loss: 0.5009 -  - ETA: 44s - loss: 0.5010 - accurac - ETA: 41s - loss: 0.5013 - accu - ETA: 40s - loss: 0.5014 - acc - ETA: 39s - loss: 0.5013 - accuracy: 0.77 - ETA: 39s - loss: 0.5014 - accuracy: 0.775 - ETA: 39s - loss: 0.5014 - accuracy: 0. - ETA: 39s - loss: 0 - ETA: 23s - loss: 0.5010 - accuracy: 0.7 - ETA: 23s - los - ETA: 21s - loss: 0.5010 - accuracy: 0.7 - ETA: 21s - loss: 0.5011 - accuracy: 0.775 - ETA: 21s - loss: 0.5011 - accurac - ETA: 20s - loss: 0.5011 - accuracy: 0.77 - ETA: 20s - loss: 0.5010 - accura - ETA: 19s - los - ETA: 18s - los - ETA: 3s - - ETA: 2s - loss: 0.5\n",
      "11284/11284 [==============================] - 119s 11ms/step\n",
      "[CV] ... dropout_rate=0.5, learn_rate=0.01, score=0.776, total=21.3min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.01 ...............................\n",
      "Epoch 1/2\n",
      "45136/45136 [==============================] - 540s 12ms/step - loss: 0.5019 - accuracy: 0.7743ETA: 1:04:50 - loss: 0.7940 - accuracy: 0.635 - ETA: 1:03:06 - loss: 0.7935 - - ETA:  - ETA: 21:25 - loss: 0.5805 -  - ETA: 20:20 - loss: 0.5741 - - ETA: 19:23 -  - ETA: 15:12 - loss: 0.5260 - ETA: 14:59 - loss:  - ETA: 14:02 - loss: 0.5207 -  - ETA: 13:49 - loss: 0.5200 - acc - ETA: 13:39 - loss: 0.52 - ETA: 13:19 - loss: 0.5189 - accuracy: 0.757 - ETA: 13:19 - loss: 0.5196 - ac - ETA: 13:12 - loss: 0.5182  - ETA: - ETA: 11:07 - loss: 0.5134 - accuracy: 0. - ETA: 11:06 - loss: 0.5136 - - ETA: 10:59 - loss: 0.5127 - accura - ETA: 10:57 - loss: 0.5132 - accuracy: - ETA: 10:54 - loss: 0.5134 - a - ETA: 10:51 - loss: 0.5108 - accuracy: 0.7 - ETA: 10:50 - loss: 0 - ETA: 10:45 - lo - ETA: 10:37 - loss:  - ETA: 10:35 - loss: 0.5102 - accuracy: 0.76 - ETA: 10:35 - loss: 0.5099 - accurac - ETA: 10:33 - loss: 0.5083 - accu - ETA: 10:32 - loss: 0.5086 - ET - ETA: 10:22 - loss: 0.5 - ETA: 9:42 - loss: 0.5086 - accuracy - ETA: 9:41 - loss: 0.5082 - accuracy: 0.76 - ETA: 9:41 - loss: 0.5085 - accuracy: 0. - ETA:  - ETA - ETA: 9:12 - loss: - ETA: 9:09 - loss: 0.5063 - accuracy - ETA: 7:14 - loss: 0.5003  - ETA: 6:57 - loss: 0.4999  - ETA: 6:55 - loss: 0.4 - ETA: 6:54 - l - ETA: 6:53 - loss: 0.4999 - accu - ETA: 6:52 - loss: 0.5003 - accuracy:  - ETA: 6:52 - l - ETA: 6:48 - loss: 0.4995 - accuracy - - ETA - ETA:  - ETA: 6:42 - loss: 0.4999 - ac - E - ETA: 6:39 - loss: - ETA:  - E - ETA: 1:19 - l - ETA: 1:17 - loss: 0.5013  - ETA: 1:16 - los - ETA: 1:14 - loss: 0.5012  - ETA: 1:13 - l - ETA: 1:12 - loss: 0.5010 -  - ETA: 1:12 - l - ETA: 1:11 - loss: - ETA: 1:11 - loss: 0.5008 - accuracy - ETA: 1:11 - los - ETA: 1:10 - loss: 0.5008 - accura - ETA: 1:10 - loss: 0.5008 - accuracy - ETA: 1:09 - ETA: 1:09 - - ETA: 52s - loss: 0.5016 - accuracy - ETA: 52s - loss: 0.50 - ETA: 51s - loss: 0.5016 - accuracy: 0.77 - ET - ETA: 49s - loss: 0.5015 - accu - ETA: 48s - loss: 0 - ETA: 47s - loss: 0.5013 - accuracy: 0.7 - ETA: 47s - loss: 0.5013  - ETA: 46s - loss: 0.5013 - accuracy: 0. - ETA: 6s - loss: 0.5018 - accuracy: 0. - ETA:  - - ETA: 1s - loss: 0.5020 - accuracy: 0. - ETA: 0s - loss: 0.5020 - accu - ETA: 0s - loss: 0.5020 - ac - ETA: 0s - loss: 0.5019 - accuracy: 0.77\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 519s 12ms/step - loss: 0.5022 - accuracy: 0.7743 1:00 - loss: 0. - ETA: 59s - loss: 0.5003 - accuracy: 0.7 - ETA: 58s - lo - ETA: 56s - loss: 0.5006 - accuracy: 0.775 - ETA: 56s - loss: 0.5006 - accura - ETA: 56s - loss: - ETA: 54s - loss: 0.5005  - ETA: 53s - loss: 0.5006  - ETA: 51s - loss: 0.5005 - accuracy: 0.775 - ETA: 51s - loss: 0.5005 - accuracy: 0.775 - ETA: 51s - loss: 0.5005 - accuracy:  - ETA: 51s - loss: 0.5005 - accuracy:  - ETA: 50s - loss: 0.5004 - accuracy: - ETA: 50s - loss: 0.5003 - accur - ETA: 49s - loss: 0. - ETA: 42s - loss: 0.5001 - accurac - ETA: 41s - - ETA: 39s - l - ETA: 34s - loss: 0.5008 - ac - ETA:  - ETA: 29s - loss: 0.500 - ETA: 25s - loss: 0.5010  - ETA: 22s - loss: 0.5014 - - ETA: 21s - loss: 0.5013 - accuracy: 0. - ETA: 21s - loss: 0.5013 - accurac - ETA: 20s - l - ETA: 18s - loss: 0.5014 - ac - ETA: 17s - loss: 0.5014 - ETA: 16s - loss: 0.5013 - accurac - ETA: 16s - lo - ETA: 4s - loss: 0.5019 - accuracy - ETA: 4s - l\n",
      "11284/11284 [==============================] - 88s 8ms/step\n",
      "[CV] ... dropout_rate=0.5, learn_rate=0.01, score=0.799, total=20.6min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.01 ...............................\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45136/45136 [==============================] - 637s 14ms/step - loss: 0.4955 - accuracy: 0.7741 8:14 - l - ETA: 8:14 - loss: 0.5067 - accuracy: 0.77 - ETA: 8:14 - loss: 0.5068 -  - ETA: 8:13 - loss: 0.5065 - accuracy: 0.77 - ETA: 8:13 - los - ETA: 8:11 - loss: - ETA: 8:06 - loss: 0.5048 - accuracy - ETA: 8:06 - l - ETA: 8:05 - loss: 0.5046 - ac - ETA: 8:05 - loss: 0.5 - ETA: 8:04 - loss: 0.5040  - ETA: 8:04 - - ETA: 8:03 - loss: 0.5034  - ETA: 8:02 - ETA:  - ETA: 7:55 - loss: 0.5024 - accuracy: 0.77 - ETA: 7:55 - - ETA: 7:54 - - ETA: 7:50 - loss: 0.502 - ETA: 7:50 - loss: 0.5026 - accura - ETA: 7:49 - loss: 0.5022 - accuracy - ETA: 7:49 - loss: 0.5022 - accuracy: 0. - ETA: 7:49 - loss: 0.5022  - ETA: 7:49 - loss: 0.5019 - accuracy - ETA: 7:48 - loss: 0.5018 - accuracy:  - ETA: 7:48 - loss: 0.502 - ETA: 7:47 - loss: 0.5023 - accuracy: 0.77 - ETA: 7:47 - loss: 0.5022 - accuracy:  - ETA: 7:47 - loss: 0.5023 - accuracy: 0.77 - ETA: 7:47 - loss: 0.5023 - ac - ETA: 7: - ETA: 7:46 - loss: 0.502 - ETA: 7:45 - loss: 0.5022 - accuracy: 0.77 - ETA: 7:45 - loss: 0.5021 - accura - ETA: 7:36 - loss: 0.500 - ETA: 7:35 - loss: 0.5006 - accura - ETA: 7:35 - loss: 0.5006 - accuracy: 0. - ETA: 7:35 - loss: 0.5005 - accu - ETA: 7:34 - loss: 0.5003 - accuracy: 0. - ETA: 7:34 - loss: 0.5002 - accura - ETA: 7:34 - loss: 0.5002 - ac - ETA: 7:33 - loss: 0.500 - ETA: 7:33 - loss: 0.5003 - accuracy - ETA: 7:33 - ETA: 7:32 - loss: 0.5006 - accura - ETA: 7:31 - loss: 0.5007 - accuracy:  - ETA: 7:31 - loss: 0.5007 - accuracy:  - ETA: 7:31 - loss: 0.5007 - accuracy: 0. - ETA: 7: - ETA: 7:30 - l - ETA: 7:29 - loss: 0.5010 - accuracy - ETA: 7:28 - loss: 0.5009 - accuracy:  - ETA: 7:28 - loss: 0.5010 - accuracy: 0.77 - ETA:  - ETA: 6:05 - loss: 0.4935 - accuracy:  - ETA: 6:05 - loss: 0.4934 - accura - ETA: 6:05 - l - ETA: 5:54 - loss: 0.4 - ETA: 5:51 - ETA: 2:28 - loss: 0.4928 - accuracy - ETA:  - ETA: 2:05 - los - - ETA: 2:02 - loss: 0.4935 - accuracy - E - ETA: 2:01 - loss: 0.4935 -  - ETA: 1:56 - loss: 0.4936 - accuracy: 0.77 - ETA: 1:56 - loss: 0.4936 - ac - ETA: 1: - ETA: 1:19 - loss: 0.4946 -  - ETA: 1:10 - E - ETA: 1:05 - - ETA: 1:04 - loss: 0.4950 - accuracy:  - ETA: 1:04 - - ETA: 1:03 - loss: 0.495 - ETA: 1:02 - loss: 0.4950 - ac - ETA: 1:02 - loss: 0.4 - ETA: 1:01 - loss: 0.4952 - accuracy: 0. - ET - ETA: 59s - loss: 0.4952  - ETA: 58s - loss: 0.4953 - accuracy: 0. - ETA: 58s - loss: 0.4953 - a - ETA: 57s - loss: 0.4955 - a - ET - ETA: 54s - loss: 0.4956 - accurac - ET - ETA: 51s - loss: 0.4953 - accuracy: 0. - ETA: 51s - loss: 0.4953 - a - ETA: 50s - loss: 0.495 - ETA: 49s - loss: 0.4 - ETA: 45s - loss: 0.4950 - accurac - ETA: 45s - loss: 0.4950 - ac - ETA: 44s - loss: 0.4951 - accuracy: 0.7 - ETA: 44s - loss: 0.4951 - accuracy:  - ETA: 43s - loss: 0.4950 -  - ETA: 42s - loss: 0.4950 - accuracy: 0. - ETA: 42s - loss: 0.4951 - - ETA: 41s - loss: 0.4951 -  - ETA: 40s - loss: 0.4 - ETA: 39s - loss: 0.4952 - ETA: 37s - loss: 0.4952 - accura - ETA - ETA: 34s  - ETA: 32s - loss: 0.4951 - accuracy: 0 - ETA: 32s - loss: 0.4951 - accuracy: 0.774 - ET - ETA: 29s - loss: 0.4951 - accuracy:  - ETA: 29s - loss: 0.4953 - accura - ETA: 28s - loss: 0.4953 - accur - ETA: 28s - loss: 0.4954 - accuracy: 0 - ETA: 27s - loss: 0.4954 - accuracy: 0. - ETA: 27s - loss: 0.4954 - accuracy: 0.7 - ETA: 27s -  - ETA: 1 - ETA: 17s - loss: 0.4951 - accuracy: 0.7 - ETA: 16s -  - ETA: 12s - loss: 0.4955 - accuracy: 0.77 - ETA: 12s - loss: 0.49\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 649s 14ms/step - loss: 0.4961 - accuracy: 0.7746 8:57 - loss: 0.5013 - accuracy: 0.76 - ETA: 9: - ETA: 8:59 - loss: 0 - ETA: 8:59 - ETA: 9:02 - loss: 0.4932 -  - ETA: 9:08 - loss: 0.4 - - ETA: 9:16 - loss: 0.4873 - ac - ETA: 9:42 - loss: 0.4822 - accura - ETA: 9:42 - loss: 0.4832 - accura - ETA: 9:43 - loss: 0.4826 - accura - ETA: 9:44 - loss: 0.4823 - accu - ETA: 9:47 - l - ETA: 9:50 - loss: 0.4 - ETA: 9:50 - loss: 0 - ETA: 9:48 - loss: 0.4808 - accuracy - ETA: 9:47 - loss: 0.4795 - accu - - ETA: 9:33 - - ETA: 9:29 - loss: 0.4862 - accuracy:  - ETA: 9:26 - loss: 0.4872 - accuracy:  - ETA: 9:23 - loss: 0.4881 - accuracy:  - ETA: 9:23 - loss: 0.4884 - ac - ETA: 9:22 - loss: 0.4885  - ETA: 9:21 - loss: - ETA - ETA: 9:18 - loss: 0.4908 - accuracy:  - ETA: 9:17 - loss: 0.4905 - ac - ETA: 9: - ETA: 9:12 - l - ETA: 9:11 - loss: 0.4905 - accuracy: 0.78 - ETA: 9:11 - ETA: 9:09 - loss: 0.4 - ETA: 9:08 - loss: - E - ETA: 9:05 - los - - - ETA: 3:03 - loss: 0.4957 - ac - ETA: 3:02 - loss: 0.4958 - accuracy:  - ETA: 3:02 - loss: 0.4958 - accuracy:  - ETA: 3:02 - loss: 0 - E - E - - ETA: 2:57 - loss: 0.4 - ETA: 2:56 - loss: 0.4958 - accuracy:  - ETA: 2:56 - loss: 0.4957 - accuracy:  - ETA: 2:56 - loss: 0.4958 - accuracy: 0.77 - ETA: 2:56 - loss: 0.495 - ETA: 2:55 - loss: 0.4957 - accu - ETA: 2:55 - loss: 0.4956 - accuracy: 0.77 - ETA: 2:55 - loss: 0.4956 - accura - - ETA: 2:53 - loss: 0.4958 - accuracy - ETA: 2:53 - loss: 0.4958 - accuracy - ETA: 2:53 - loss: 0.4958 - ac - ETA: 2:52 - loss: 0.4957 - accu - ETA: 2:51 - - ETA: 2:50 - loss: 0 - ETA: 2:49 - loss: 0.4963 -  - ETA: 2:48 - loss: 0.4963 - accuracy:  - ETA: 2:48 - loss: 0.4963 - accuracy: 0.77 - ETA: 2:48 - loss: 0.496 - ETA: 2:48 - loss: 0.4963  - ETA: 2:47 - loss: 0.4963 - accura - - ETA: 1:25 - loss: 0.4973 - accuracy:  - ETA: 1:25 - loss: - ETA: 1:23 - loss: 0.4975 - accuracy: 0.77 - ETA: 1:12 - loss: 0.4975 - accuracy: 0.77 - ETA: 1:11 - los - ETA: 58s - loss: 0.4976 - accura - ET - ETA: 55s - loss: 0.4976  - ETA: 54s  - ETA: 47s - loss: 0.4973 - accuracy: 0.7 - ETA: 46s - loss: 0.4972 - accura - E - ETA: 43s - loss: 0.49 - ETA: 37 - ETA: 35s - loss: 0.4972  - ETA: 34s - loss: 0.4972 - -  - ETA: 28s - loss: 0.4970 - accuracy: 0.7 - ETA: 28s - loss: 0.4969 - ETA: 27 - ETA: 25s - loss: 0.4971 - accu - ETA: 24s - loss: 0.4970 - accuracy: 0.77 - ETA: 24s - loss: 0.4970 - accuracy: 0.774 - ETA: 24s - loss: 0.497 - ETA: 22s - loss: 0.4970  - ETA:  - ETA: 19s - loss: 0.4968 - accuracy: 0.7 - ETA: 19s - loss: 0.4967 - accurac - ETA: 19s - loss: 0.4967 - acc - ETA: 18s - loss: 0.4967 - accuracy:  - ETA: 17s - loss: 0.4966 - accuracy: 0.7 - ETA: 17s - loss: 0.4966 - accuracy: 0. - ETA: 17s - loss: 0.4966 - accuracy: 0.77 - ETA: 17s - loss: 0.4966 - accura - ETA: 16s - loss: 0.4966 - accuracy: 0.7 - ETA: 16s - loss: 0.4965 - accurac - ETA: 16s - loss:  - ETA: 14s - loss: 0.4 - ETA: 13s - loss: 0.4965 -  - E\n",
      "11284/11284 [==============================] - 118s 10ms/step\n",
      "[CV] ... dropout_rate=0.5, learn_rate=0.01, score=0.783, total=24.7min\n",
      "[CV] dropout_rate=0.5, learn_rate=0.01 ...............................\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45136/45136 [==============================] - 690s 15ms/step - loss: 0.4965 - accuracy: 0.7739 1:44 - loss: 0.4949 - ac - ETA: 1:44 - loss: 0.4949 - accura - ETA: 1:40 - loss: 0.495 - ETA: 1:26 - loss: 0.4954 - accuracy:  - ETA: 1:25 - ETA: 1:24 - loss: 0.4953  - ETA:  - ETA: 1:11 - loss: 0.4957 - accuracy: 0.77 - ETA: 1: - ETA: 1:05 - loss: - ETA: 1: - ETA: 1:00 - loss: 0.4959 - a - ETA: 59s - loss: 0. - ETA: 57s - loss: 0.4959 - accuracy: 0 - E - ETA: 49s - loss: 0.4957 - accuracy: 0.77 - ETA: 49s - loss: 0.4956  - ETA: 39s - loss: 0.4956 - ETA: 37s - loss: 0.4957 -  - ETA: 36s - loss: 0.4956 - accuracy: 0 - ETA: 36s - loss: 0.4957 - accuracy: 0.774 - ETA: 36s - loss: - ETA: 34s - loss: 0.4958  - ETA: 33s - loss: 0.4957 - accuracy: 0.7 - ETA: 3 - ETA: 31s - loss: - ETA: 29s - loss: 0.49 - ETA: 22s - loss: 0.4961 - accuracy: 0. - ETA: 2 -  - ETA: 1 - ETA: 11s - loss: 0.4960 - acc -  - ETA: 0s - loss: 0.4964 -  - ETA: 0s - loss: 0.4965 - accuracy: 0.77\n",
      "Epoch 2/2\n",
      "45136/45136 [==============================] - 660s 15ms/step - loss: 0.5079 - accuracy: 0.7764s\n",
      "11284/11284 [==============================] - 121s 11ms/step\n",
      "[CV] ... dropout_rate=0.5, learn_rate=0.01, score=0.777, total=26.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed: 730.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "56420/56420 [==============================] - 905s 16ms/step - loss: 0.3672 - accuracy: 0.8720ETA: 7:24:00 - loss: 0.7799 - - ETA: 2:52:13 -  - ETA: 1:24:16 - loss: 0.8678 - accu - ETA: 1:11:51 - loss: 0.859 - ETA: 57:5 - ETA: 31:06 - loss: 0.7523 - accuracy:  - ETA: 30:34 - loss: 0.7519 - ac - ETA: 29:34 - loss: 0.7479 - accuracy: 0 - ETA: 29:14 - loss: 0.7467 - accu - ETA: 28:32 - loss: 0.74 - ETA: 27:32 - loss: 0.7404 - accuracy: 0. - ETA: 27:20 - loss:  - ETA: 26 - ETA: 25:01 - loss: 0.7264  - ETA: 2 - ETA: 23:2 - ETA: 22: - ETA: 18:23 - loss: 0.6092 - ac - ETA: 18:21 - loss: 0.6086 - accuracy: 0 - ETA: 17:58 - loss: 0.5950 - accuracy: 0.67 - ETA: 1 - ETA: 17:48 - loss:  - ETA: 17:43 - loss: 0.5909 - accu - ETA: 17:40 - loss: 0 - ETA: 17:33 - loss: 0.5870 - ETA: 16:55 - loss: 0.5784  - ETA: 16:50 - loss: 0.5767 - accurac - ETA: 16:48 - loss: 0.5761 - accuracy: 0. - ETA: 16:47  - ETA: 16:40 - loss: 0.5 - ETA: 16:35 - loss: 0.5736 - ETA: 16:31 - loss: 0.5720 - accuracy: 0.703 - ETA: 16:31 - loss: 0. - ETA: 16:26 - loss: 0.5698 - accuracy: - ETA: 16:24 - lo - ETA: 16:19 - loss: 0.5675 - accuracy: 0.70 - ETA: 16:19 - loss - ETA: 15:56 - loss - ETA: 15:50 - loss: 0.5583 - accuracy: 0. - ETA: 15:50 - loss: 0.5584 - accuracy - ETA: 15:49 - loss: 0.5579 - accur - ETA: 1:06 - loss: 0 - ETA: 1:05 - loss: 0.3738 - accuracy: 0.86 - ETA: 1:05 - loss: 0.373 - ETA: 1:00 - loss: 0.3734  - ETA: 1:00 - loss: 0.3733 - accuracy: 0.86 - ETA: 1:00 - loss: 0.373 - ETA: 58s - loss: 0.3731 - accuracy: 0. - ET - ETA: 55 - ETA: 53s - loss: 0.3722 - accuracy: 0.8 - ETA: 53s - loss: 0.3721 - accuracy: 0.8 - ETA: 53s - lo - ETA: 51s - loss: - ETA: 49s - loss: 0.3718  - ETA: 48s - loss: 0.3716 - accuracy: 0 - ETA: 48s - loss: 0.3716 - a - ETA: 47s - loss: 0.3715 - accuracy: 0.86 - ETA: 46s - loss: 0.3715 - - ETA: 45s - loss: 0.37 - ETA: 44s - loss - ETA: 42s - loss: 0.3 - ETA: 41s - loss - ETA: 39s - loss: 0.3710 - accuracy: 0 - ETA: 39s - loss: 0.3709 - accuracy: 0.869 - ETA: 39s - loss: 0.3709 - accuracy: 0.869 - ETA: 39s - loss:  -  - ETA: 28s - loss: 0.3695 - accurac - ETA: 28s  - ETA - ETA: 20s - loss: 0.3686 - acc - ETA: 19s - loss: 0.3 - ETA: 1s -\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14350/56420 [======>.......................] - ETA: 10:41 - loss: 0.2631 - accuracy: 0.9382- ETA: 16:11 - loss: 0.1686 - accuracy: 0.9 - ETA: 15:33 - loss: 0.1626 - accuracy: 0.956 - ETA: 15:27 - loss: 0.1543 - accuracy: - ETA: 14:59 - loss: 0.298 - ETA: 14:09 - loss: 0.2208 - accuracy - ETA: 14:21 - loss - ETA: 15:07 - loss: 0.2444 - accuracy: 0 - ETA: 15:13 - lo - ETA: 15:26 - loss: 0.2816 - accuracy: 0.943 - ETA: 15:26 - loss: 0.2922 - accuracy: 0.93 - ETA: 15:25 - loss: 0.2886 - accuracy: 0.93 - ETA: 15:23 - loss: 0.2863 - accuracy: 0. - ETA: 15:21 - loss: 0.2814 - accu - ETA: 15:10 - loss:  - ETA: 15:01 - loss: 0.2621 - accuracy: 0.9 - ETA: 14 - ETA: 14:48 - loss: 0.2607 - ac - ETA: 14: - ETA: 14:36 - loss: 0.289 - ETA: 14:31 - loss: 0.2998 - accuracy: 0.92 - ETA: 14:30 - loss: 0.2988 - accuracy - ETA: 14:23 - loss: 0.2909 - accuracy: 0.92 - ETA: 14:23 - ETA: 14:20 - lo - ETA: 14:14 - loss: 0. - ETA: 14:12 - loss: 0.2836 - accurac - ETA: 14:11 - loss: 0.2805 - - ETA: 14:10 - loss: 0.2812 - accuracy: 0.933 - ETA: 14:10 - lo - ETA: 14:07 - loss: 0.2722 -  - ETA: 14:04 - loss: 0.2716 - accuracy: 0.935 - ETA: 14:04 - loss: 0.2713 - accura - ETA: 14:03 - loss: 0.2717 - accuracy: 0.936 - ETA: 14:03 - loss: 0.2713 - accuracy: 0 - ETA: 14:02 - loss: 0.2695 - accuracy: 0.93 - ETA: 14:02 - loss: 0.2687 - accuracy - ETA: 14:01 -  - ETA: 13:58 - loss: 0.2772 - accuracy:  - ETA: 13:57 - loss: 0.2758 - accuracy: 0. - ETA: 13:57 - loss: 0.2754 - ETA: 13:56 - loss: 0.2745 - acc - ETA: 13:55 - loss: 0.2764 - accuracy: - ETA: 13:50 - loss: 0.2768 - - ETA: 13:50 - loss:  - ETA: 13:47 - loss: 0.2748 - accuracy: 0.934 - ETA: 13:47 - loss: 0.2746 - accuracy:  - ETA: 13:47 - loss: 0.2752 - accuracy: - ETA: 13:46 - loss: 0.2740 - accuracy: - ETA: 13:46 - loss: 0.2747 - accuracy: 0.934 - ETA: 13:46 - loss: 0.2745 - accurac - ETA: 13:45 - loss: 0.2745 - - ETA: 13:43 - - ETA: 13:40 - loss: 0 - ETA: 13:37 - loss: 0.2764 - accuracy - ETA: 13:37 - l - ETA: 13:29 - loss: 0.2760 - accuracy:  - ETA: 13:29 - loss: 0.2753 - accuracy: 0. - ETA: 13:28 - loss: 0.2750 - accuracy: 0 - ETA - ETA: 13:25 - l - ETA: 13:22 - loss: 0.2779 - accuracy: 0.93 - ETA: 13:22 - loss: 0.2775 - accuracy:  - ETA: 13:21 - loss: 0.27 - ETA: 13:38 - loss: 0.2747 - accura - ETA: 13:37 - loss: 0.2768 - accuracy: 0.93 - ETA: 13:37 - loss: 0.2765 - ac - ETA: 13:36 - loss: 0.2756 - accura - ETA: 13:35 - loss: 0.2744  - ETA: 13:34 - loss: 0.2732 - accuracy: 0.935 - ETA: 13:34 - loss: 0.27 - ETA: 13:31 - loss: 0.2710 - accuracy: 0.935 - ETA: 13:31 - loss: 0.2709 - accuracy:  - ETA: 13:31 - loss: 0.2701 - accuracy: 0.9 - ETA: 13:30 - loss: 0.2710 - accuracy: 0.93 - ETA: 13:30 - loss: 0.2710 - accuracy - ETA: 13:29 - loss: 0.2703 - acc - ETA: 13:28 - loss: 0.2708 - accuracy: 0.935 - ETA: 13:28 - loss: - ETA: 13:25 - loss: 0.2708 - accuracy: 0. - - ETA: 13:20 - loss: 0.2723 - accuracy: 0 - ETA: 13:19 - loss: 0.2718 - accuracy: 0 - ETA: 13:18 - loss: 0.2722 - accurac - ETA: 13:18 - loss: 0.2716 - accurac - ETA: 13:17 - loss: 0.2706 - accuracy: 0.93 - ETA: 13:17 - loss: 0.2703 - accura - ETA: 13:16 - loss: - ETA: 13:13 - loss: 0.2691 - accura - ETA: 13:12 - loss: 0.2706 - accuracy: 0 - ETA: 13:12 - loss: 0.2701 - accura - ETA: 13:11 - loss: 0.2693 - accuracy: 0.9 - ETA: 13:11 - loss: 0.2690 - accuracy: 0.9 - ETA: 13:10 - loss: 0.2704 - accuracy: 0.93 - ETA: 13:10 - loss: 0. - ETA: 13:08 - loss: 0.2688 - accurac - ETA: 13:07 - loss: 0.2696 - accuracy: - ET - ETA: 13:04 - loss: 0.2670 - accura - ETA: 13:03 - loss: 0.2678 - accur - ETA: 13:02 - loss: 0.2696 - accura - ETA: 13:01 - loss: 0.2 - ETA: 12:59 - loss: 0.2722 - accuracy: 0. - ETA: 12:59 - loss: 0.2717 - accurac - ETA: 12:58 - loss: 0.2719 - accuracy: - ETA: 12:58 - - ETA: 12:55 - loss: 0.2726 - accu - ETA: 12:54 - loss: 0 - ETA: 12:52 - loss: 0.2753 - acc - ETA: 12:51 - loss: 0.2751 - - ETA: 12:50 - loss: 0.2737 - accur - ETA: 12:49 - loss: 0.2735 - accuracy: 0.934 - ETA: 12:49 - loss: 0.2733 - accuracy: 0 - ETA: 12:49 - loss: 0.2730  - ETA: 12:47 - loss: 0.2724 - accuracy: 0.934 - ETA: 12:47 - loss: 0.2730 - accuracy: - ETA: 12:47 - lo - ETA: 12:45 - loss: 0.2731 - accurac - ETA: 12:44 - loss: 0.2723 - accuracy: - ETA: 12:44 - loss: 0.2719 - - ETA: 12:42 - loss: 0.2732 - accuracy: 0 - ETA: 12:42 - loss: 0.2735 - accuracy - ETA: 12:41 - loss: 0.2741 - accuracy: 0.9 - ETA: 12:41 - loss: 0.27 - ETA: 12:39 - loss: 0.2756 - - ETA: 12:38 - loss: 0.2769 - accuracy: 0 - ETA: 12:38 - loss: 0.2765 - accuracy: 0. - ETA: 12:37 - loss: 0.2761 - accur - ETA: 12:36 - loss: - ETA: 12: - ETA: 12:32 - loss: 0.2778  - ETA: 12:31 - loss: 0.2764  - ETA: 12:29 - loss: 0.2757 - accuracy - ETA: 12:29 - loss: 0.2750 - accu - ETA: 12:28 - loss: 0.2740 - accu - ETA: 12:27 - lo - ETA: 12:25 - loss: 0.2746 - accura - ETA: 12:25 - loss: 0.2 - ETA: 12:20 - loss: 0.2747 - accuracy: 0.9 - ETA: 12:20 - loss: 0.2751 - acc - ETA: 12:19 - loss: 0.2747 - accuracy: 0.934 - ETA: 12:19 - loss: 0.2747 - - ETA: 12:18 - loss: 0.2737 - accuracy - ETA: 12:17 - loss: 0.2731  - ETA: 12:15 - loss: 0.2725 - accuracy: - ETA: 12:15 - loss: 0. - ETA: 12:13 - lo - ETA: 12:11 - loss: 0.2731 - accuracy:  - ETA: 12:11 -  - ETA: 12:08 - loss - ETA: 12:06 - loss: 0.2715 - accuracy: 0.935 - ETA: 12:06 - loss: 0.2714 - accura - ETA: 12:05 - loss: 0.2720 - accuracy: 0.9 - ETA: 12:05 - loss: 0.2718 - accuracy: 0.93 - ETA: 12:05 - loss: 0.2723 - accuracy: 0 - ETA: 12:05 -  - ETA: 12:02  - ETA: 12:00 - loss: 0.271 - ETA: 11:59 - loss: 0.2722 - accurac - ETA: 11:58 - loss: 0.2722 - accuracy: 0.93 - ETA: 11:58 - loss: 0.2721 - accuracy:  - ETA: 11:57 - loss: 0.2724 - accurac - ETA: 11:57 - loss: 0.2719 - - ETA: 11:56 - loss: 0.2709 - accuracy: 0.93 - ETA: 11:55 - loss: 0.2708 - a - ETA - ETA: 11:45 - loss: 0.2673 - accurac - ETA: 11:45 - loss: 0.26 - ETA: 11:44 - loss: 0.2676 - accuracy: 0.936 - ETA: 11:44 - loss - ETA: 11:42 - loss: 0.268 - ETA: 11:41 - loss: 0.2677 - accuracy - ETA: 11:40 - loss: 0.2675 - accurac - ETA: 11:40 - loss: 0.2682 - accuracy: 0. - ETA: 11:40 - loss: 0.2680 - accu - ETA: 11:39 - loss: 0.2679 - accuracy:  - ETA: 11:38 - loss: 0.2676 - ac - ETA: 11:38 - loss: 0.2668 - accuracy: 0 - ETA: 11:37 - loss: 0.267 - ETA: 11:36 - los - ETA: 11:34 - loss: 0.2665 - accuracy: - ETA: 11:33 - loss: 0.2662 - accurac - ETA: 11:33 - loss: 0.2662 - accuracy:  - ETA: 11:32 - loss: 0.2665 - accuracy: - ETA: 11:32 - loss: 0.2667 - ac - ETA: - ETA: 11:28 - loss: 0.2652 - accuracy: 0. - ETA: 11:27 - loss: 0.2654 - accuracy: 0. - ETA: 11:27 - lo - ETA: 11:25 - loss - ETA: 11:23 - loss: 0.2668 - accuracy: 0.936 - ET - ETA: 11:20 - loss: 0.2666  - ETA: 11:19 - loss: 0.2671 - accuracy:  - ETA: 11:19 - loss: 0.2673 - accuracy: 0 - ETA: 11:19 - loss: 0.2671 - ac - ETA: 11:18 - loss: 0.2669 - a - ETA: 11:17 - loss: 0.2675 -  - ETA: 11:16 - loss: 0.266 - ETA: 11:15 - loss: 0.2671 - accuracy: 0.93 - ETA: 11:14 - loss: 0.2670 - accu - ETA: 11:14 - loss: 0.2664 - accuracy: 0.93 - ETA: 11: - ETA: 11:11 - loss: 0.2651 - accur - ETA: 11:10 - loss: 0.2645 - accuracy: 0 - ETA: 11:10 - loss: 0.2643  - ETA: 11:09 - loss: 0.2643 - accuracy: 0.937 - ETA: 11:09 - loss: 0.2643 - accuracy: 0. - ETA: 11:08 - loss: 0.2641 - accuracy: - ETA: 11:08 - loss: 0.2638 - accu - ETA: 11:07 - loss: 0.2643 - accu - ETA: 11:06 - loss: 0.2638 - accuracy: 0.937 - ETA: 11:06 - loss: 0.2642 - accuracy: 0. - ETA: 11:06 - loss: 0.2641 - accurac - ETA: 11:06 - loss: 0.2 - ETA: 11:04 - loss: 0.2638 - accur - ETA: 11:03 - loss: 0.2632 - accur - ETA: 11:03 - loss: 0.2637 - accura - ETA: 11 - ETA - ETA: 10:57 - loss: 0.2662 - accuracy: 0.93 - ETA: 10:57 - loss: 0.2669  - ETA: 10:55 - loss: 0.2670 - accuracy: 0.937 - ETA: 10:55 - loss: 0.2670 - accurac - ETA: 10:55 - loss: 0.2668 - accur - ETA: 10:54 - loss: 0.2663 - accuracy: 0.93 - ETA: 10:54 - loss: 0.2661 - accuracy: 0 - ETA: 10:54 - loss: 0.2659 - accuracy: 0.9 - ETA: 10:53 - loss: 0.2658 - accura - ETA: 10:53 - loss: 0.2 - ETA: 10:51 - loss: 0.2660 - accur - ETA: 10:50 - loss: 0.2656  -  - ETA: 10:45 - loss: 0.2654 - accuracy: 0.93 - ETA: 10:45 - loss: 0.2653 - accuracy: 0 - ETA: 10:44 - loss: 0.2652 - ETA: 10:43 - loss:  - ETA: 10:42 - loss: 0.2633 - accuracy:  - ETA: 10:41 - loss: 0.2634 - accuracy: 0.9381"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56420/56420 [==============================] - 871s 15ms/step - loss: 0.2520 - accuracy: 0.9366 59s - loss: 0.2532 - accuracy: - ETA: 59s - lo - ETA: 57s - loss: 0.2532 - ac - ETA: 56s - loss: 0. - ETA: 1s - loss: 0.2 - ETA: 0s - loss: 0.2521 - accuracy: 0. - ETA: 0s - loss: 0.2521 - ac\n",
      "Best: 0.9512584328651428, using {'dropout_rate': 0.0, 'learn_rate': 0.0001}\n",
      "0.9512584328651428 (0.019367617059615278) with: {'dropout_rate': 0.0, 'learn_rate': 0.0001}\n",
      "0.8510280132293702 (0.023144309763626664) with: {'dropout_rate': 0.0, 'learn_rate': 0.001}\n",
      "0.791368305683136 (0.0066892030048304385) with: {'dropout_rate': 0.0, 'learn_rate': 0.01}\n",
      "0.8832683444023133 (0.013217713520297594) with: {'dropout_rate': 0.25, 'learn_rate': 0.0001}\n",
      "0.8586848497390747 (0.008106494597631396) with: {'dropout_rate': 0.25, 'learn_rate': 0.001}\n",
      "0.7994328260421752 (0.02047936877151605) with: {'dropout_rate': 0.25, 'learn_rate': 0.01}\n",
      "0.873183274269104 (0.010626685626809584) with: {'dropout_rate': 0.5, 'learn_rate': 0.0001}\n",
      "0.8511698126792908 (0.01309117428384063) with: {'dropout_rate': 0.5, 'learn_rate': 0.001}\n",
      "0.7840482115745544 (0.008270666182909944) with: {'dropout_rate': 0.5, 'learn_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Optimize Droprate and Learning Rate\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "def initalizeModel(learn_rate, dropout_rate):\n",
    "    # Get sequenctial keras model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Layer\n",
    "    model.add(Dense(128, input_dim = 22, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Hidden Layer 1\n",
    "    model.add(Dense(64, input_dim = 128, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    #  Hidden Layer 2\n",
    "    model.add(Dense(32, input_dim = 64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(1, input_dim = 32, activation='sigmoid'))\n",
    "    \n",
    "    # Set the learning rate\n",
    "    adam = Adam(lr = learn_rate)\n",
    "    \n",
    "    # Compile the Model\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Defining a random seed\n",
    "randomSeed = 2\n",
    "np.random.seed(randomSeed)\n",
    "\n",
    "model = KerasClassifier(build_fn = initalizeModel, epochs = 2, batch_size = 2, verbose = 1) \n",
    "\n",
    "learn_rate = [0.0001, 0.001, 0.01]\n",
    "dropout_rate = [0.0, 0.25, 0.5]\n",
    "\n",
    "# make a dictionary of grid seacrh params\n",
    "parameterGrid = dict(learn_rate=learn_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "\n",
    "# Build and Fit\n",
    "grid = GridSearchCV(estimator = model, param_grid = parameterGrid, cv = KFold(random_state= randomSeed), refit = True, verbose = 10)\n",
    "\n",
    "grid_results = grid.fit(inStandardized, outData)\n",
    "\n",
    "# Result Display\n",
    "print(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('{0} ({1}) with: {2}'.format(mean, stdev, param))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56420/56420 [==============================] - 510s 9ms/step\n",
      "Actual Data: [0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0]\n",
      "Predictions: [0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0]\n",
      "Overall Accuracy Score: 0.9719248493442042\n",
      "\n",
      "               Classification Report\n",
      "=====================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     43993\n",
      "           1       0.97      0.90      0.93     12427\n",
      "\n",
      "    accuracy                           0.97     56420\n",
      "   macro avg       0.97      0.95      0.96     56420\n",
      "weighted avg       0.97      0.97      0.97     56420\n",
      "\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "# Get some detailed reuslts\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "predictions = grid.predict(inStandardized)\n",
    "blankList = []\n",
    "for element in predictions:\n",
    "    for value in element:\n",
    "        blankList.append(value)\n",
    "blankList = np.array(blankList)\n",
    "print(\"Actual Data: {0}\".format(outData[:20]))\n",
    "print(\"Predictions: {0}\".format(blankList[:20]))\n",
    "print(\"Overall Accuracy Score: {0}\".format(accuracy_score(outData, blankList)))\n",
    "print()\n",
    "print(\"               Classification Report\")\n",
    "print(\"=====================================================\")\n",
    "print(classification_report(outData, blankList))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize our hyperparameter tuning!\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "\n",
    "def plotHyperParameters(cv_results, x, z, metric='mean_test_score'):\n",
    "    cv_results = pd.DataFrame(cv_results)\n",
    "    col_x = 'param_' + x\n",
    "    col_z = 'param_' + z\n",
    "    figurePlot, ax = plt.subplots(1, 1, figsize=(11, 8))\n",
    "    seaborn.pointplot(x=col_x, y=metric, hue=col_z, data=cv_results, ci=99, n_boot=64, ax=ax)\n",
    "    ax.set_title(\"Hyperparamter Tuning (Learn Rate & Dropout Rate)\")\n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.legend(title=z)\n",
    "    return figurePlot\n",
    "figurePlot = plotHyperParameters(grid.cv_results_, 'learn_rate', 'dropout_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
